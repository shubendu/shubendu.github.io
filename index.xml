<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Shubendu Biswas</title>
        <link>https://shubendu.github.io/</link>
        <description>Recent content on Shubendu Biswas</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 09 Sep 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://shubendu.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Chinese Test</title>
        <link>https://shubendu.github.io/p/test-chinese/</link>
        <pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/test-chinese/</guid>
        <description>&lt;img src="https://shubendu.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash.jpg" alt="Featured image of post Chinese Test" /&gt;&lt;h2 id=&#34;正文测试&#34;&gt;正文测试&lt;/h2&gt;
&lt;p&gt;而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。&lt;/p&gt;
&lt;p&gt;奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。&lt;/p&gt;
&lt;h2 id=&#34;引用&#34;&gt;引用&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;思念是最暖的忧伤像一双翅膀&lt;br&gt;
让我停不了飞不远在过往游荡&lt;br&gt;
不告而别的你 就算为了我着想&lt;br&gt;
这么沉痛的呵护 我怎么能翱翔&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=3aypp_YlBzI&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;最暖的憂傷 - 田馥甄&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;图片&#34;&gt;图片&lt;/h2&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 66; flex-basis: 160px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-chinese/florian-klauer-nptLmg6jqDo-unsplash.jpg&#34; data-size=&#34;667x1000&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-chinese/florian-klauer-nptLmg6jqDo-unsplash.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-chinese/florian-klauer-nptLmg6jqDo-unsplash_hu595aaf3b3dbbb41af5aed8d3958cc9f9_13854_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-chinese/florian-klauer-nptLmg6jqDo-unsplash_hu595aaf3b3dbbb41af5aed8d3958cc9f9_13854_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;667&#34;
				height=&#34;1000&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Photo by Florian Klauer on Unsplash&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Photo by Florian Klauer on Unsplash&lt;/figcaption&gt;
		
	&lt;/figure&gt;  &lt;figure style=&#34;flex-grow: 149; flex-basis: 359px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-chinese/luca-bravo-alS7ewQ41M8-unsplash.jpg&#34; data-size=&#34;1000x667&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-chinese/luca-bravo-alS7ewQ41M8-unsplash.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-chinese/luca-bravo-alS7ewQ41M8-unsplash_hu0a3f1163de68d0b9471979ebf0ecf11e_32400_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-chinese/luca-bravo-alS7ewQ41M8-unsplash_hu0a3f1163de68d0b9471979ebf0ecf11e_32400_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1000&#34;
				height=&#34;667&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Photo by Luca Bravo on Unsplash&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Photo by Luca Bravo on Unsplash&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 133; flex-basis: 320px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash.jpg&#34; data-size=&#34;1000x750&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1000&#34;
				height=&#34;750&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Photo by Helena Hertz on Unsplash&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Photo by Helena Hertz on Unsplash&lt;/figcaption&gt;
		
	&lt;/figure&gt;  &lt;figure style=&#34;flex-grow: 66; flex-basis: 160px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-chinese/hudai-gayiran-3Od_VKcDEAA-unsplash.jpg&#34; data-size=&#34;667x1000&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-chinese/hudai-gayiran-3Od_VKcDEAA-unsplash.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-chinese/hudai-gayiran-3Od_VKcDEAA-unsplash_hub241c2a9c7a2caf7e16a2a5bbc7141ff_18711_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-chinese/hudai-gayiran-3Od_VKcDEAA-unsplash_hub241c2a9c7a2caf7e16a2a5bbc7141ff_18711_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;667&#34;
				height=&#34;1000&#34;
				loading=&#34;lazy&#34;
				alt=&#34;Photo by Hudai Gayiran on Unsplash&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;Photo by Hudai Gayiran on Unsplash&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;![&lt;span class=&#34;nt&#34;&gt;Photo by Florian Klauer on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;florian-klauer-nptLmg6jqDo-unsplash.jpg&lt;/span&gt;)  ![&lt;span class=&#34;nt&#34;&gt;Photo by Luca Bravo on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;luca-bravo-alS7ewQ41M8-unsplash.jpg&lt;/span&gt;) 

![&lt;span class=&#34;nt&#34;&gt;Photo by Helena Hertz on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;helena-hertz-wWZzXlDpMog-unsplash.jpg&lt;/span&gt;)  ![&lt;span class=&#34;nt&#34;&gt;Photo by Hudai Gayiran on Unsplash&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;hudai-gayiran-3Od_VKcDEAA-unsplash.jpg&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;相册语法来自 &lt;a class=&#34;link&#34; href=&#34;https://typlog.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Typlog&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Sentiment analysis of tweets</title>
        <link>https://shubendu.github.io/p/sentiment-analysis-of-tweets/</link>
        <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/sentiment-analysis-of-tweets/</guid>
        <description>&lt;h1 id=&#34;problem&#34;&gt;Problem&lt;/h1&gt;
&lt;p&gt;Twitter data is massive and as such analyzing twitter data is a mammoth undertaking. the cleaning and pre-processing of Twitter. Political polarization and reactions to new products are probably some of the biggest use-cases of twitter data analytics. We have dataset containing user tweets. These tweets can be negative (0) and positive (4). Based on the obtained data our goal is to identify the sentiment/polarity of the tweets.&lt;/p&gt;
&lt;h1 id=&#34;about-dataset&#34;&gt;About Dataset&lt;/h1&gt;
&lt;p&gt;There are 6 columns/features in the dataset and are described below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;target: the polarity of the tweet (0 = negative, 4 = positive)&lt;/li&gt;
&lt;li&gt;ids: The id of the tweet ( ex :2087)&lt;/li&gt;
&lt;li&gt;date: the date of the tweet (ex: Sat May 16 23:58:44 UTC 2009)&lt;/li&gt;
&lt;li&gt;flag: The query (lyx). If there is no query, then this value is NO_QUERY.&lt;/li&gt;
&lt;li&gt;user: the user-name of the user that tweeted&lt;/li&gt;
&lt;li&gt;text: the text of the tweet&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;importing-libraries&#34;&gt;Importing libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support as error_metric
from nltk.corpus import stopwords
import nltk
from string import punctuation
from nltk.stem.porter import *
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;eda&#34;&gt;EDA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;loading-dataset-checking-for-null-values-and-sampling-from-data&#34;&gt;Loading Dataset ,checking for null values and sampling from data&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first step - you know the drill by now - load the dataset and see how it looks like. In this task, we are basically looking for glimpse of overall data and look at the null values if they are present. Since the data 1600000 rows, We will be taking sample of 5000 tweets for the sake of easy computation power.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;columns = [&#39;target&#39;,&#39;ids&#39;,&#39;date&#39;,&#39;flag&#39;,&#39;user&#39;,&#39;text&#39;]
data = pd.read_csv(&#39;file.csv&#39;, encoding = &#39;latin-1&#39;, header = None)
data.columns = columns
data.shape
data = data.sample(n=5000, random_state=2)
data
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/1.jpg&#34; alt=&#34;describe&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;sentiment-analysis-simplified&#34;&gt;Sentiment Analysis simplified&lt;/h2&gt;
&lt;p&gt;In layman&amp;rsquo;s terms sentiment analysis is basically figuring out the sentiment of a particular text. In the case with our problem statement, it is about finding out if a particular tweet is positive or negative, happy or unhappy (based on a score of course).&lt;/p&gt;
&lt;p&gt;Approach towards dealing with sentiment analysis&lt;/p&gt;
&lt;p&gt;The key to sentiment analysis is finding a number which indicates a level of positivity or negativity usually a number between 0 to 1, as it is obvious that computers cannot understand emotions such as happy or sad. However, a number could substitute an emotion and the value of this number signifies the magnitude of emotion in a direction; ex: very happy is above 0.9, happy is between 0.75 and 0.9, neutral is 0.5 and so on. The main challenge is how to obtain this number correctly with the help of given data.&lt;/p&gt;
&lt;p&gt;In supervised setting you already have information as labels are already present in the training phase. First you would clean the data of unwanted substances like stopwords, punctuation marks etc. and then extract features by converting words into numbers. You can do this extraction step in various different ways like Bag-of-Words, TF-IDF with or without n-grams approach. Then you feed some sort of classifier which outputs a labels based on some internal scoring mechanism.&lt;/p&gt;
&lt;h2 id=&#34;why-use-sentiment-analysis&#34;&gt;Why use sentiment analysis?&lt;/h2&gt;
&lt;p&gt;It’s estimated that 80% of the world’s data is unstructured and not organized in a pre-defined manner. Most of this comes from text data, like emails, support tickets, chats, social media, surveys, articles, and documents. These texts are usually difficult, time-consuming and expensive to analyze, understand, and sort through.&lt;/p&gt;
&lt;p&gt;Sentiment analysis systems allows companies to make sense of this sea of unstructured text by automating business processes, getting actionable insights, and saving hours of manual data processing, in other words, by making teams more efficient.&lt;/p&gt;
&lt;p&gt;With the help of sentiment analysis, unstructured information could be automatically transformed into structured data of public opinions about products, services, brands, politics, or any topic that people can express opinions about. This data can be very useful for commercial applications like marketing analysis, public relations, product reviews, net promoter scoring, product feedback, and customer service.&lt;/p&gt;
&lt;h2 id=&#34;issues-with-sentiment-analysis&#34;&gt;Issues with sentiment analysis&lt;/h2&gt;
&lt;p&gt;Computers have a hard time carrying out sentiment analysis tasks. Some pressing issues with performing sentiment analysis are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sarcasm: It is one of the most difficult sentiments to interpret properly. Example: &amp;ldquo;It was awesome for the week that it worked.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Relative sentiment: It is not a classic negative, but can be a negative nonetheless. Example: &amp;ldquo;I bought an iPhone&amp;rdquo; is good for Apple, but not for other mobile companies.&lt;/li&gt;
&lt;li&gt;Compound or multidimensional sentiment: These types contain positives and negatives in the same phrase. Example: &amp;ldquo;I love Mad Men, but hate the misleading episode trailers.&amp;rdquo; The above sentence consists of two polarities, i.e., Positive as well as Negative. So how do we conclude whether the review was Positive or Negative?&lt;/li&gt;
&lt;li&gt;Use of emoticons: Heavy use of emoticons (which have sentiment values) in social media texts like that of Twitter and Facebook also makes text analysis difficult.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-preprocessing&#34;&gt;Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;tackling-user-handles&#34;&gt;Tackling user handles&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tweets can be directed to any other person with username/user handle &amp;ldquo;NAME&amp;rdquo; @NAME. Consider the tweet &amp;lsquo;@brodiejay OH IM GOING THERE! Wow Mona Vale is a real place afterall! I know it sucks Mville only does the slow train pffft &amp;lsquo;. Here, &amp;lsquo;@brodiejay&amp;rsquo; is the user handle or username of the person to whom that particular tweet was directed/referred. If you take another tweet &amp;ldquo;my baby&amp;rsquo;s growing up &amp;ldquo;, it doesn&amp;rsquo;t contain any such user handle. So, in the dataset, user handles are present, but not in all observations.&lt;/p&gt;
&lt;p&gt;From basic intuition it is clear that user handles have little to zero contribution towards sentiment formation. So, it is a good idea to remove them altogether from the data. Getting rid of user handles also helps in reduction of the term-frequency matrix that gets generated. This will directly boost up calculation speed and also performance as unnecessary features will not be generated. Below code will clean our tweets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, &#39;&#39;, input_txt)

    return input_txt  
data[&#39;clean_text&#39;] = data[&#39;text&#39;].apply(lambda row:remove_pattern(row, &amp;quot;@[\w]*&amp;quot;))

data.head(5)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/2.jpg&#34; alt=&#34;clean-tweet&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;text-preprocessing&#34;&gt;Text Preprocessing&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This data can now be further preprocessed of unwanted entries which will make the data more suitable for carrying out analysis. Below discussed in brief are the preprocessing techniques that you will be carrying out at the end of this topic.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tokenization- Tokenization describes splitting paragraphs into sentences, or sentences into individual pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens. Most of what we are going to do with language relies on ﬁrst separating out such tokens separately so that subsequent preprocessing steps can be successfully done.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stopwords removal- Stopwords are the most common words in a language like &amp;lsquo;the&amp;rsquo;, &amp;lsquo;a&amp;rsquo;, &amp;lsquo;on&amp;rsquo;, &amp;lsquo;is&amp;rsquo;, &amp;lsquo;all&amp;rsquo;. These words do not carry important meaning and so are usually removed from texts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stemming- Much of natural language machine learning is about sentiment of the text. Stemming is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look). The main two algorithms are Porter stemming algorithm (removes common morphological and inflexional endings from words and Lancaster stemming algorithm (a more aggressive stemming algorithm).
Below Code will tokenize our clean text then removing stopwords from nltk, after that we will aply stemming.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;stop_words = list(set(stopwords.words(&#39;english&#39;)))+list(punctuation)+[&#39;``&#39;, &amp;quot;&#39;s&amp;quot;, &amp;quot;...&amp;quot;, &amp;quot;n&#39;t&amp;quot;]
data[&#39;tokenized_text&#39;] = [nltk.word_tokenize(x) for x in data[&#39;clean_text&#39;]]
data[&#39;tokenized_text&#39;] = data[&#39;tokenized_text&#39;].apply(lambda row: [word for word in row if word not in stop_words])
stemmer = PorterStemmer()
data[&#39;tokenized_text&#39;] = data[&#39;tokenized_text&#39;].apply(lambda x: [stemmer.stem(i) for i in x])
data[&#39;tokenized_text&#39;] = data[&#39;tokenized_text&#39;].apply(lambda x: &#39; &#39;.join(x))
data.head()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/3.jpg&#34; alt=&#34;clean-tweet&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As you can see tokenized_text&amp;rsquo;s 1st row is converted into &amp;ldquo;OH IM go there&amp;rdquo; from &amp;ldquo;OH IM GOING THERE!&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;word-cloud&#34;&gt;Word Cloud&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;word-cloud-of-all-data&#34;&gt;Word cloud of all data&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is a visualisation method that displays how frequently words appear in a given body of text, by making the size of each word proportional to its frequency. All the words are then arranged in a cluster or cloud of words. Alternatively, the words can also be arranged in any format: horizontal lines, columns or within a shape.&lt;/p&gt;
&lt;p&gt;Now that you have an idea about how to use a wordcloud, lets do a simple task to generate a wordcloud to observe the most frequently occuring words.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;all_words = &#39; &#39;.join([text for text in data[&#39;tokenized_text&#39;]])
# generate wordcloud object
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)
plt.figure(figsize=(20, 12))
plt.imshow(wordcloud, interpolation=&amp;quot;bilinear&amp;quot;)
plt.axis(&#39;off&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/4.jpg&#34; alt=&#34;word-cloud&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;word-cloud-for-negative-data&#34;&gt;Word cloud for negative data&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# negative tweets
neg_words = &#39; &#39;.join([text for text in data[&#39;tokenized_text&#39;][data[&#39;target&#39;] == 0]])

# generate wordcloud object for negative tweets
neg_wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(neg_words)
plt.figure(figsize=(20, 12))
plt.imshow(neg_wordcloud, interpolation=&amp;quot;bilinear&amp;quot;)
plt.axis(&#39;off&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/5.jpg&#34; alt=&#34;word-cloud-negative&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;word-cloud-for-postive-data&#34;&gt;Word cloud for postive data&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# positive tweets
pos_words = &#39; &#39;.join([text for text in data[&#39;tokenized_text&#39;][data[&#39;target&#39;] == 4]])

# generate wordcloud object for negative tweets
pos_wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(pos_words)
plt.figure(figsize=(20, 12))
plt.imshow(pos_wordcloud, interpolation=&amp;quot;bilinear&amp;quot;)
plt.axis(&#39;off&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/6.jpg&#34; alt=&#34;word-cloud-postive&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As you can see in negative word cloud there are lots of negative words such as fuck, bad, tire etc and in postive word cloud there are many good words.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;model-building&#34;&gt;Model Building&lt;/h2&gt;
&lt;p&gt;Logistic Regression for predicting sentiments
These steps enabled us with data in proper format as well as some interesting insights about certain keywords. This cleaned form of data can now be fed to a machine learning algorithm to classify tweets into categories. Since we are already familiar with the workflow of classifying texts, we will be only outlining the steps associated with it. The steps are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Splitting into training and test sets&lt;/li&gt;
&lt;li&gt;Construct a term-document matrix (can be done by Bag of Words or TF-IDF)&lt;/li&gt;
&lt;li&gt;Fitting a classifier on training data&lt;/li&gt;
&lt;li&gt;Predicting on test data&lt;/li&gt;
&lt;li&gt;Evaluating classifier performance&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# ratio to split into training and test set
ratio = int(len(data)*0.75)

# logistic regression model
logreg = LogisticRegression(random_state=2)

# TF-IDF feature matrix
tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words=&#39;english&#39;)

# fit and transform tweets
tweets = tfidf_vectorizer.fit_transform(data[&#39;tokenized_text&#39;])

# convert positive tweets to 1&#39;s
data[&#39;target&#39;] = data[&#39;target&#39;].apply(lambda x: 1 if x==4 else x)

# split into train and test
X_train = tweets[:ratio,:]
X_test = tweets[ratio:,:]
y_train = data[&#39;target&#39;].iloc[:ratio]
y_test = data[&#39;target&#39;].iloc[ratio:]

# fit on training data
logreg.fit(X_train,y_train)

# make predictions
prediction = logreg.predict_proba(X_test)
prediction_int = prediction[:,1] &amp;gt;= 0.3
prediction_int = prediction_int.astype(np.int)
report = classification_report(y_test,prediction_int)

# print out classification_report
print(report)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/7.jpg&#34; alt=&#34;report&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;textblob&#34;&gt;TextBlob&lt;/h2&gt;
&lt;p&gt;There are numerous libraries out there for dealing with text data. One of them is TextBlob which is built on the shoulders of NLTK and Pattern. A big advantage of TextBlob is it is easy to learn and offers a lot of features like sentiment analysis, pos-tagging, noun phrase extraction, etc. It has now become a go-to library for many users who are performing NLP tasks.&lt;/p&gt;
&lt;p&gt;Features of TextBlob&lt;/p&gt;
&lt;p&gt;The documentation page of TextBlob says; TextBlob aims to provide access to common text-processing operations through a familiar interface. You can treat TextBlob objects as if they were Python strings that learned how to do Natural Language Processing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;sentiment-analysis-with-textblob&#34;&gt;Sentiment Analysis with TextBlob&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can carry out sentiment analysis with textBlob too. The TextBlob object has an attribute sentiment that returns a tuple of the form Sentiment (polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0] with negative values corresponding to negative sentiments and positive values to positive sentiments. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.&lt;/p&gt;
&lt;p&gt;But how it does this prediction of sentiment scoring? Well, it has a training set with preclassified movie reviews, so when you give a new text for analysis, it uses NaiveBayes classifier to classify the new text&amp;rsquo;s polarity in pos and neg probabilities.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# list to store polarities
tb_polarity = []

# loop over tweets
for sentence in data[&#39;tokenized_text&#39;]:
    temp = TextBlob(sentence)
    tb_polarity.append(temp.sentiment[0])

# new column to store polarity    
data[&#39;tb_polarity&#39;] = tb_polarity
data.head(10)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/8.jpg&#34; alt=&#34;TextBlob&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can compare the target values with tb_polarity and see how much TextBlob is accurate&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;vadersentiment&#34;&gt;VaderSentiment&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/9.jpg&#34; alt=&#34;vader&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Another library for out of the box sentiment analysis is vaderSentiment. It is an open sourced python library where VADER stands for Valence Aware Dictionary and sEntiment Reasoner. With VADER you can be up and running performing sentiment classification very quickly even if you don&amp;rsquo;t have positive and negative text examples to train a classifier or want to write custom code to search for words in a sentiment lexicon. VADER is also computationally efficient when compared to other Machine Learning and Deep Learning approaches.&lt;/p&gt;
&lt;p&gt;VADER performs well on text originating in social media and is described fully in a paper entitled VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. published at ICWSM-14. VADER is able to include sentiment from emoticons (e.g, :-)), sentiment-related acronyms (e.g, LOL) and slang (e.g, meh). The developers of VADER have used Amazon’s Mechanical Turk to get most of their ratings, You can find complete details on their Github Page.&lt;/p&gt;
&lt;h3 id=&#34;output-of-vader&#34;&gt;Output of VADER&lt;/h3&gt;
&lt;p&gt;VADER produces four sentiment metrics from these word ratings. The first three metrics; positive, neutral and negative, represent the proportion of the text that falls into those categories. The final metric, the compound score, is the sum of all of the lexicon ratings which have been standardised to range between -1 and 1.&lt;/p&gt;
&lt;h3 id=&#34;how-does-vader-work&#34;&gt;How does VADER work?&lt;/h3&gt;
&lt;p&gt;First step is to import SentimentIntensityAnalyzer from vaderSentiment.vaderSentiment. Then initialize an object of SentimentIntensityAnalyzer() and use its .polarity_scores() on a given text to find about its four metrics. Below given is a code snippet:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;analyser = SentimentIntensityAnalyzer()

# empty list to store VADER polarities
vs_polarity = []

# loop over tweets
for sentence in data[&#39;tokenized_text&#39;]:
    vs_polarity.append(analyser.polarity_scores(sentence)[&#39;compound&#39;])

# add new column `&#39;vs_polarity&#39;` to data
data[&#39;vs_polarity&#39;] = vs_polarity

data.head(10)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/10.jpg&#34; alt=&#34;vader1&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;you can also compare the vs_polarity to the target variables.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-and-data&#34;&gt;Code and Data&lt;/h2&gt;
&lt;p&gt;Full Code and Dataset can be found &lt;a class=&#34;link&#34; href=&#34;https://github.com/shubendu/Twitter-Analysis&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/h2&gt;
&lt;p&gt;Congratulations if you have reached her :) We can try tree based models such as random forest and GBDT and other types of feature transformation such as word2vec. If You have any question regarding this blog feel free to contact me on my website&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://greyatom.com/&#34;&gt;https://greyatom.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Loan Defaulters</title>
        <link>https://shubendu.github.io/p/loan-defaulters/</link>
        <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/loan-defaulters/</guid>
        <description>&lt;h1 id=&#34;problem&#34;&gt;Problem&lt;/h1&gt;
&lt;p&gt;For this project we will be exploring the publicly available data from LendingClub.com. Lending Club connects people who need money (borrowers) with people who have money (investors). As an investor one would want to invest in people who showed a profile of having a high probability of paying the amount back. Using Decision Tree model, classify whether or not the borrower paid back their loan in full.&lt;/p&gt;
&lt;h1 id=&#34;about-dataset&#34;&gt;About Dataset&lt;/h1&gt;
&lt;p&gt;The snapshot of the data we will be working on:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/1.jpg&#34; alt=&#34;1&#34;  /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;customer.id&lt;/td&gt;
&lt;td&gt;ID of the customer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;credit.policy&lt;/td&gt;
&lt;td&gt;If the customer meets the credit underwriting criteria of LendingClub.com or not&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;purpose&lt;/td&gt;
&lt;td&gt;The purpose of the loan(takes values :&amp;ldquo;creditcard&amp;rdquo;, &amp;ldquo;debtconsolidation&amp;rdquo;, &amp;ldquo;educational&amp;rdquo;, &amp;ldquo;majorpurchase&amp;rdquo;, &amp;ldquo;smallbusiness&amp;rdquo;, and &amp;ldquo;all_other&amp;rdquo;).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;int.rate&lt;/td&gt;
&lt;td&gt;The interest rate of the loan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;installment&lt;/td&gt;
&lt;td&gt;The monthly installments owed by the borrower if the loan is funded&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;log.annual.inc&lt;/td&gt;
&lt;td&gt;The natural log of the self-reported annual income of the borrower&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dti&lt;/td&gt;
&lt;td&gt;The debt-to-income ratio of the borrower (amount of debt divided by annual income)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fico&lt;/td&gt;
&lt;td&gt;The FICO credit score of the borrower&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;days.with.cr.line&lt;/td&gt;
&lt;td&gt;The number of days the borrower has had a credit line.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;revol.bal&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s revolving balance (amount unpaid at the end of the credit card billing cycle)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;revol.util&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s revolving line utilization rate (the amount of the credit line used relative to total credit available)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pub.rec&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s number of derogatory public records (bankruptcy filings, tax liens, or judgments)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;inq.last.6mths&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s number of inquiries by creditors in the last 6 months&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;delinq.2yrs&lt;/td&gt;
&lt;td&gt;The number of times the borrower had been 30+ days past due on a payment in the past 2 years&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;paid.back.loan&lt;/td&gt;
&lt;td&gt;Whether the user has paid back loan&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;importing-libraries&#34;&gt;Importing libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;loading-dataset-checking-for-null-values&#34;&gt;Loading Dataset ,checking for null values&lt;/h2&gt;
&lt;p&gt;The first step - you know the drill by now - load the dataset and see how it looks like. In this task, we are basically looking for glimpse of overall data and look at the null values if they are present also some statistical representation of our data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data  = pd.read_csv(&#39;loan.csv&#39;)
data.head().T
data.describe()
data.info()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/2.jpg&#34; alt=&#34;2&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/3.jpg&#34; alt=&#34;3&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/4.jpg&#34; alt=&#34;4&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have no null values. We&amp;rsquo;ll drop the customer id, as it is of no use for our model and we have both numeric and categorical types of data which we will further preprocess.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;splitting-the-data&#34;&gt;Splitting the data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s split the data into train and test&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;X = data.drop([&#39;customer.id&#39;,&#39;paid.back.loan&#39;], axis = 1)
y = data[&#39;paid.back.loan&#39;]
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state = 0)
print(X_train.shape , y_train.shape)
print(X_test.shape, y_test.shape)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/5.jpg&#34; alt=&#34;5&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;target-variable-distribution&#34;&gt;Target variable distribution&lt;/h2&gt;
&lt;p&gt;The distribution of &amp;ldquo;paid.back.loan&amp;rdquo; and plotting barplot.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/6.jpg&#34; alt=&#34;6&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can see that 5639 people have paid back loan while 1065 people not paid back the loan.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;feature-enginnering&#34;&gt;Feature Enginnering&lt;/h2&gt;
&lt;p&gt;We need to preprocess data beofre feature engineering as we can see that &amp;ldquo;int.rate&amp;rdquo; column has percentage symbol which need to be remove and later I am dividing that column with 100 to get the actual percentage values. After that I will be seperating the data into numeric and categorical dataframe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Removing the last character from the values in column
X_train[&#39;int.rate&#39;] = X_train[&#39;int.rate&#39;].map(lambda x: str(x)[:-1])

#Dividing the column values by 100
X_train[&#39;int.rate&#39;]=X_train[&#39;int.rate&#39;].astype(float)/100

#Removing the last character from the values in column
X_test[&#39;int.rate&#39;] = X_test[&#39;int.rate&#39;].map(lambda x: str(x)[:-1])

#Dividing the column values by 100
X_test[&#39;int.rate&#39;]=X_test[&#39;int.rate&#39;].astype(float)/100

#Storing all the numerical type columns in &#39;num_df&#39;
num_df=X_train.select_dtypes(include=[&#39;number&#39;]).copy()

#Storing all the categorical type columns in &#39;cat_df&#39;
cat_df=X_train.select_dtypes(include=[&#39;object&#39;]).copy()
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;feature-visualisation&#34;&gt;Feature Visualisation&lt;/h2&gt;
&lt;p&gt;Now we can visualise the distribuiton of our numeric dataset in different calss variable. Below code will do the job&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cols=list(num_df.columns)
for i in range(9):          
    
    #Plotting boxplot
    sns.boxplot(x=y_train,y=num_df[cols[i]],ax=axes[i])
    
    #Avoiding subplots overlapping
    fig.tight_layout()    
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/7.jpg&#34; alt=&#34;7&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/8.jpg&#34; alt=&#34;8&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Most of our features has different distribution for our class variable which is good for our model&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;Lets&amp;rsquo;s visualise the categorical features as well. I will be plotting using seaborn to see how our distribution differs in different class.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cols=list(cat_df.columns)
#Looping through rows
for i in range(0,2):
    
    #Looping through columns
    for j in range(0,2):
        
        #Plotting count plot
        sns.countplot(x=X_train[cols[i*2+j]], hue=y_train,ax=axes[i,j])                        
        
        #Avoiding subplots overlapping
        fig.tight_layout()    

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/9.jpg&#34; alt=&#34;9&#34;  /&gt;
&lt;img src=&#34;https://shubendu.github.io/img/loan/10.jpg&#34; alt=&#34;10&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can see that the major reason that stands common for the majority of customers who have applied for a loan is debt_consolidation which means taking one loan to payoff there other loans.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;model-building&#34;&gt;Model Building&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s Apply the Decision Tree classifier to our dataset. We will encode the categorical features using label encoder.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for col in cat_df.columns:
    
    #Filling null values with &#39;NA&#39;
    X_train[col].fillna(&#39;NA&#39;,inplace=True)
    
    #Initalising a label encoder object
    le=LabelEncoder()
    
    #Fitting and transforming the column in X_train with &#39;le&#39;
    X_train[col]=le.fit_transform(X_train[col]) 
    
    #Filling null values with &#39;NA&#39;
    X_test[col].fillna(&#39;NA&#39;,inplace=True)
    
    #Fitting the column in X_test with &#39;le&#39;
    X_test[col]=le.transform(X_test[col]) 

# Replacing the values of y_train
y_train.replace({&#39;No&#39;:0,&#39;Yes&#39;:1},inplace=True)

# Replacing the values of y_test
y_test.replace({&#39;No&#39;:0,&#39;Yes&#39;:1},inplace=True)

#Initialising &#39;Decision Tree&#39; model    
model=DecisionTreeClassifier(random_state=0)

#Training the &#39;Decision Tree&#39; model
model.fit(X_train, y_train)

#Finding the accuracy of &#39;Decision Tree&#39; model
acc=model.score(X_test, y_test)

#Printing the accuracy
print(acc)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/11.jpg&#34; alt=&#34;11&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have total 74% accuracy on our model without having any hyperparameter tuning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;decision-tree-pruning&#34;&gt;Decision Tree Pruning&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s see if pruning of decision tree improves its accuracy. We will use grid search to do the optimum pruning.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;parameter_grid = {&#39;max_depth&#39;: np.arange(3,10), &#39;min_samples_leaf&#39;: range(10,50,10)}

#Code starts here

#Initialising &#39;Decision Tree&#39; model
model_2 = DecisionTreeClassifier(random_state=0)

#Applying Grid Search of hyper-parameters and finding the optimum &#39;Decision Tree&#39; model
p_tree = GridSearchCV(model_2, parameter_grid, cv=5)

#Training the optimum &#39;Decision Tree&#39; model
p_tree.fit(X_train, y_train)

#Finding the accuracy of the optimum &#39;Decision Tree&#39; model
acc_2 = p_tree.score(X_test, y_test)

#Printing the accuracy
print(acc_2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/12.jpg&#34; alt=&#34;12&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Great our accuracy has improved drastically.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;tree-visualising&#34;&gt;Tree visualising&lt;/h2&gt;
&lt;p&gt;we can also visualise our tree.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Importing header files

from io import StringIO
from sklearn.tree import export_graphviz
from sklearn import tree
from sklearn import metrics
from IPython.display import Image
import pydotplus


#Creating DOT data
dot_data = export_graphviz(decision_tree=p_tree.best_estimator_, out_file=None, 
                                feature_names=X.columns, filled = True,  
                                class_names=[&#39;loan_paid_back_yes&#39;,&#39;loan_paid_back_no&#39;])

#Drawing graph
graph_big = pydotplus.graph_from_dot_data(dot_data)  

#Displaying graph
# show graph - do not delete/modify the code below this line

img_path = user_data_dir+&#39;/file.png&#39;
graph_big.write_png(img_path)

plt.figure(figsize=(20,15))
plt.imshow(plt.imread(img_path))
plt.axis(&#39;off&#39;)
plt.show() 

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/13.jpg&#34; alt=&#34;13&#34;  /&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-and-data&#34;&gt;Code and Data&lt;/h2&gt;
&lt;p&gt;Full Code and Dataset can be found &lt;a class=&#34;link&#34; href=&#34;https://github.com/shubendu/Loan-Defaulters&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/h2&gt;
&lt;p&gt;Congratulations if you have reached her :) We can try GBDT and XgBoost to increase our model accuracy.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://greyatom.com/&#34;&gt;https://greyatom.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>sLoan Defaulters inside one</title>
        <link>https://shubendu.github.io/p/sloan-defaulters-inside-one/</link>
        <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/sloan-defaulters-inside-one/</guid>
        <description>&lt;h1 id=&#34;problem&#34;&gt;Problem&lt;/h1&gt;
&lt;p&gt;For this project we will be exploring the publicly available data from LendingClub.com. Lending Club connects people who need money (borrowers) with people who have money (investors). As an investor one would want to invest in people who showed a profile of having a high probability of paying the amount back. Using Decision Tree model, classify whether or not the borrower paid back their loan in full.&lt;/p&gt;
&lt;h1 id=&#34;about-dataset&#34;&gt;About Dataset&lt;/h1&gt;
&lt;p&gt;The snapshot of the data we will be working on:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/1.jpg&#34; alt=&#34;1&#34;  /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;customer.id&lt;/td&gt;
&lt;td&gt;ID of the customer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;credit.policy&lt;/td&gt;
&lt;td&gt;If the customer meets the credit underwriting criteria of LendingClub.com or not&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;purpose&lt;/td&gt;
&lt;td&gt;The purpose of the loan(takes values :&amp;ldquo;creditcard&amp;rdquo;, &amp;ldquo;debtconsolidation&amp;rdquo;, &amp;ldquo;educational&amp;rdquo;, &amp;ldquo;majorpurchase&amp;rdquo;, &amp;ldquo;smallbusiness&amp;rdquo;, and &amp;ldquo;all_other&amp;rdquo;).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;int.rate&lt;/td&gt;
&lt;td&gt;The interest rate of the loan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;installment&lt;/td&gt;
&lt;td&gt;The monthly installments owed by the borrower if the loan is funded&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;log.annual.inc&lt;/td&gt;
&lt;td&gt;The natural log of the self-reported annual income of the borrower&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dti&lt;/td&gt;
&lt;td&gt;The debt-to-income ratio of the borrower (amount of debt divided by annual income)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fico&lt;/td&gt;
&lt;td&gt;The FICO credit score of the borrower&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;days.with.cr.line&lt;/td&gt;
&lt;td&gt;The number of days the borrower has had a credit line.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;revol.bal&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s revolving balance (amount unpaid at the end of the credit card billing cycle)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;revol.util&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s revolving line utilization rate (the amount of the credit line used relative to total credit available)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pub.rec&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s number of derogatory public records (bankruptcy filings, tax liens, or judgments)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;inq.last.6mths&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s number of inquiries by creditors in the last 6 months&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;delinq.2yrs&lt;/td&gt;
&lt;td&gt;The number of times the borrower had been 30+ days past due on a payment in the past 2 years&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;paid.back.loan&lt;/td&gt;
&lt;td&gt;Whether the user has paid back loan&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;importing-libraries&#34;&gt;Importing libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;loading-dataset-checking-for-null-values&#34;&gt;Loading Dataset ,checking for null values&lt;/h2&gt;
&lt;p&gt;The first step - you know the drill by now - load the dataset and see how it looks like. In this task, we are basically looking for glimpse of overall data and look at the null values if they are present also some statistical representation of our data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data  = pd.read_csv(&#39;loan.csv&#39;)
data.head().T
data.describe()
data.info()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/2.jpg&#34; alt=&#34;2&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/3.jpg&#34; alt=&#34;3&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/4.jpg&#34; alt=&#34;4&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have no null values. We&amp;rsquo;ll drop the customer id, as it is of no use for our model and we have both numeric and categorical types of data which we will further preprocess.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;splitting-the-data&#34;&gt;Splitting the data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s split the data into train and test&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;X = data.drop([&#39;customer.id&#39;,&#39;paid.back.loan&#39;], axis = 1)
y = data[&#39;paid.back.loan&#39;]
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state = 0)
print(X_train.shape , y_train.shape)
print(X_test.shape, y_test.shape)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/5.jpg&#34; alt=&#34;5&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;target-variable-distribution&#34;&gt;Target variable distribution&lt;/h2&gt;
&lt;p&gt;The distribution of &amp;ldquo;paid.back.loan&amp;rdquo; and plotting barplot.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/6.jpg&#34; alt=&#34;6&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can see that 5639 people have paid back loan while 1065 people not paid back the loan.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;feature-enginnering&#34;&gt;Feature Enginnering&lt;/h2&gt;
&lt;p&gt;We need to preprocess data beofre feature engineering as we can see that &amp;ldquo;int.rate&amp;rdquo; column has percentage symbol which need to be remove and later I am dividing that column with 100 to get the actual percentage values. After that I will be seperating the data into numeric and categorical dataframe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Removing the last character from the values in column
X_train[&#39;int.rate&#39;] = X_train[&#39;int.rate&#39;].map(lambda x: str(x)[:-1])

#Dividing the column values by 100
X_train[&#39;int.rate&#39;]=X_train[&#39;int.rate&#39;].astype(float)/100

#Removing the last character from the values in column
X_test[&#39;int.rate&#39;] = X_test[&#39;int.rate&#39;].map(lambda x: str(x)[:-1])

#Dividing the column values by 100
X_test[&#39;int.rate&#39;]=X_test[&#39;int.rate&#39;].astype(float)/100

#Storing all the numerical type columns in &#39;num_df&#39;
num_df=X_train.select_dtypes(include=[&#39;number&#39;]).copy()

#Storing all the categorical type columns in &#39;cat_df&#39;
cat_df=X_train.select_dtypes(include=[&#39;object&#39;]).copy()
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;feature-visualisation&#34;&gt;Feature Visualisation&lt;/h2&gt;
&lt;p&gt;Now we can visualise the distribuiton of our numeric dataset in different calss variable. Below code will do the job&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cols=list(num_df.columns)
for i in range(9):          
    
    #Plotting boxplot
    sns.boxplot(x=y_train,y=num_df[cols[i]],ax=axes[i])
    
    #Avoiding subplots overlapping
    fig.tight_layout()    
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/7.jpg&#34; alt=&#34;7&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/8.jpg&#34; alt=&#34;8&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Most of our features has different distribution for our class variable which is good for our model&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;Lets&amp;rsquo;s visualise the categorical features as well. I will be plotting using seaborn to see how our distribution differs in different class.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cols=list(cat_df.columns)
#Looping through rows
for i in range(0,2):
    
    #Looping through columns
    for j in range(0,2):
        
        #Plotting count plot
        sns.countplot(x=X_train[cols[i*2+j]], hue=y_train,ax=axes[i,j])                        
        
        #Avoiding subplots overlapping
        fig.tight_layout()    

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/9.jpg&#34; alt=&#34;9&#34;  /&gt;
&lt;img src=&#34;https://shubendu.github.io/img/loan/10.jpg&#34; alt=&#34;10&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can see that the major reason that stands common for the majority of customers who have applied for a loan is debt_consolidation which means taking one loan to payoff there other loans.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;model-building&#34;&gt;Model Building&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s Apply the Decision Tree classifier to our dataset. We will encode the categorical features using label encoder.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for col in cat_df.columns:
    
    #Filling null values with &#39;NA&#39;
    X_train[col].fillna(&#39;NA&#39;,inplace=True)
    
    #Initalising a label encoder object
    le=LabelEncoder()
    
    #Fitting and transforming the column in X_train with &#39;le&#39;
    X_train[col]=le.fit_transform(X_train[col]) 
    
    #Filling null values with &#39;NA&#39;
    X_test[col].fillna(&#39;NA&#39;,inplace=True)
    
    #Fitting the column in X_test with &#39;le&#39;
    X_test[col]=le.transform(X_test[col]) 

# Replacing the values of y_train
y_train.replace({&#39;No&#39;:0,&#39;Yes&#39;:1},inplace=True)

# Replacing the values of y_test
y_test.replace({&#39;No&#39;:0,&#39;Yes&#39;:1},inplace=True)

#Initialising &#39;Decision Tree&#39; model    
model=DecisionTreeClassifier(random_state=0)

#Training the &#39;Decision Tree&#39; model
model.fit(X_train, y_train)

#Finding the accuracy of &#39;Decision Tree&#39; model
acc=model.score(X_test, y_test)

#Printing the accuracy
print(acc)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/11.jpg&#34; alt=&#34;11&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have total 74% accuracy on our model without having any hyperparameter tuning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;decision-tree-pruning&#34;&gt;Decision Tree Pruning&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s see if pruning of decision tree improves its accuracy. We will use grid search to do the optimum pruning.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;parameter_grid = {&#39;max_depth&#39;: np.arange(3,10), &#39;min_samples_leaf&#39;: range(10,50,10)}

#Code starts here

#Initialising &#39;Decision Tree&#39; model
model_2 = DecisionTreeClassifier(random_state=0)

#Applying Grid Search of hyper-parameters and finding the optimum &#39;Decision Tree&#39; model
p_tree = GridSearchCV(model_2, parameter_grid, cv=5)

#Training the optimum &#39;Decision Tree&#39; model
p_tree.fit(X_train, y_train)

#Finding the accuracy of the optimum &#39;Decision Tree&#39; model
acc_2 = p_tree.score(X_test, y_test)

#Printing the accuracy
print(acc_2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/12.jpg&#34; alt=&#34;12&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Great our accuracy has improved drastically.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;tree-visualising&#34;&gt;Tree visualising&lt;/h2&gt;
&lt;p&gt;we can also visualise our tree.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Importing header files

from io import StringIO
from sklearn.tree import export_graphviz
from sklearn import tree
from sklearn import metrics
from IPython.display import Image
import pydotplus


#Creating DOT data
dot_data = export_graphviz(decision_tree=p_tree.best_estimator_, out_file=None, 
                                feature_names=X.columns, filled = True,  
                                class_names=[&#39;loan_paid_back_yes&#39;,&#39;loan_paid_back_no&#39;])

#Drawing graph
graph_big = pydotplus.graph_from_dot_data(dot_data)  

#Displaying graph
# show graph - do not delete/modify the code below this line

img_path = user_data_dir+&#39;/file.png&#39;
graph_big.write_png(img_path)

plt.figure(figsize=(20,15))
plt.imshow(plt.imread(img_path))
plt.axis(&#39;off&#39;)
plt.show() 

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/loan/13.jpg&#34; alt=&#34;13&#34;  /&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-and-data&#34;&gt;Code and Data&lt;/h2&gt;
&lt;p&gt;Full Code and Dataset can be found &lt;a class=&#34;link&#34; href=&#34;https://github.com/shubendu/Loan-Defaulters&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/h2&gt;
&lt;p&gt;Congratulations if you have reached her :) We can try GBDT and XgBoost to increase our model accuracy.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://greyatom.com/&#34;&gt;https://greyatom.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Human Activity Recognition with Smartphones</title>
        <link>https://shubendu.github.io/p/human-activity-recognition-with-smartphones/</link>
        <pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/human-activity-recognition-with-smartphones/</guid>
        <description>&lt;h1 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h1&gt;
&lt;p&gt;The Human Activity Recognition database was built from the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. The objective is to classify activities into one of the six activities performed.&lt;/p&gt;
&lt;h1 id=&#34;about-dataset&#34;&gt;About Dataset&lt;/h1&gt;
&lt;p&gt;The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKINGUPSTAIRS, WALKINGDOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.&lt;/p&gt;
&lt;p&gt;The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.&lt;/p&gt;
&lt;h1 id=&#34;attribute-information&#34;&gt;Attribute information&lt;/h1&gt;
&lt;p&gt;For each record in the dataset the following is provided:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Triaxial Angular velocity from the gyroscope.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A 561-feature vector with time and frequency domain variables.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Its activity label.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An identifier of the subject who carried out the experiment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;importing-libraries&#34;&gt;Importing libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import precision_recall_fscore_support as error_metric
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;1-eda&#34;&gt;1. EDA&lt;/h2&gt;
&lt;h3 id=&#34;11-loading-dataset-checking-for-null-values-and-statistical-description&#34;&gt;1.1 Loading Dataset ,checking for null values and statistical description&lt;/h3&gt;
&lt;p&gt;The first step - you know the drill by now - load the dataset and see how it looks like. In this task, we are basically looking for descriptive statistics of overall data and look at the null values if they are present. Since the data is too large I will be only printiing till 25 columns.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data = pd.read_csv(&#39;file.csv&#39;)
print(&#39;Null Values In DataFrame: {}\n&#39;.format(data.isna().sum().sum()))
data.describe().T[:25]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/1.jpg&#34; alt=&#34;describe&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;We have 563 features and 10299 rows&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/2.jpg&#34; alt=&#34;describe-2&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After observing the dataset statistical description majority of the values are having a minimum value of -1 and maximum value of 1 which brings us to the conclusion that scaling is not required in this case. Also the data set is free of any kind of null values.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;12-distribution-of-our-class&#34;&gt;1.2 Distribution of our class&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s get a little bit deeper dive into exploring the target feature via graphical methods.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;label = data[&#39;Activity&#39;]
sns.countplot(x= label)
plt.xticks(rotation=75);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/3.jpg&#34; alt=&#34;class-distribution&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The class distribution are somewhat balanced which is a better thing on prediction and model building part.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;13-how-long-does-the-participant-use-the-staircase&#34;&gt;1.3 How Long Does The Participant Use The Staircase?&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since the dataset has been created in an scientific environment nearly equal preconditions for the participants can be assumed. It is highly likely for the participants to have been walking up and down the same number of staircases. Bascially we are looking to calculate how much time the each participant has been spending while walking downstairs and upstairs as it seems to be most frequent activity. Let us investigate their activity durations. The description of the data states; &amp;ldquo;fixed-width sliding windows of 2.56 sec and 50% overlap&amp;rdquo; for each datapoint.Therefore a single datapoint is gathered every 1.28 sec.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data_copy = data.copy() 
data_copy[&#39;duration&#39;] = &#39;&#39;
duration_df = (data_copy.groupby([label[label.isin([&#39;WALKING_UPSTAIRS&#39;, &#39;WALKING_DOWNSTAIRS&#39;])], &#39;subject&#39;])[&#39;duration&#39;].count() * 1.28)
duration_df = pd.DataFrame(duration_df)
plot_data = duration_df.reset_index().sort_values(&#39;duration&#39;, ascending=False)
plot_data[&#39;Activity&#39;] = plot_data[&#39;Activity&#39;].map({&#39;WALKING_UPSTAIRS&#39;:&#39;Upstairs&#39;, &#39;WALKING_DOWNSTAIRS&#39;:&#39;Downstairs&#39;})
plt.figure(figsize=(15,5))
sns.barplot(data=plot_data, x=&#39;subject&#39;, y=&#39;duration&#39;, hue=&#39;Activity&#39;)
plt.title(&#39;Participants Compared By Their Staircase Walking Duration&#39;)
plt.xlabel(&#39;Participants&#39;)
plt.ylabel(&#39;Total Duration [s]&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/4.jpg&#34; alt=&#34;plot4&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Nearly all participants have more data for walking upstairs than downstairs. Assuming an equal number of up- and down-walks the participants need longer walking upstairs. Furthermore the range of the duration is narrow and adjusted to the conditions. A young person being ~50% fast in walking upstairs than an older one is reasonable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;14-finding-the-correlation&#34;&gt;1.4 Finding the correlation&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We always try to reduce our features to the minimum number of most significant features. The basic rule of feature selection is that we need to select features which are highly correlated to the dependent variable and also not highly correlated with each other as they show the same trend. Correlation refers to the mutual relationship and association between quantities and it is generaly used to express one quantity in terms of its relationship with other quantities. This can either be Positive(variables change in the same direction), negative(variables change in opposite direction or neutral(No correlation). Let&amp;rsquo;s investigate the correlated pairs. We are storing the correlated pairs exculding self correlated pairs in the variable named as top_corr_feilds with a threshold of 0.8 for absolute correlation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
feature_cols = data.columns[: -2]   
correlated_values = data[feature_cols].corr()
correlated_values = (correlated_values.stack().to_frame().reset_index().rename(columns={&#39;level_0&#39;: &#39;Feature_1&#39;, &#39;level_1&#39;: &#39;Feature_2&#39;, 0:&#39;Correlation_score&#39;}))
correlated_values[&#39;abs_correlation&#39;] = correlated_values.Correlation_score.abs()
top_corr_fields = correlated_values.sort_values(&#39;Correlation_score&#39;, ascending = False).query(&#39;abs_correlation&amp;gt;0.8 &#39;)
top_corr_fields = top_corr_fields[top_corr_fields[&#39;Feature_1&#39;] != top_corr_fields[&#39;Feature_2&#39;]].reset_index(drop=True)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/5.jpg&#34; alt=&#34;plot5&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-data-processing-and-baseline-model&#34;&gt;2. Data Processing and baseline model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;21-data-preparation-and-implementing-baseline-model&#34;&gt;2.1 Data preparation and implementing baseline model&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, we will be building a baseline model of svm which would take data with insignificant amount of data preprocessing which is to observe how our model is performing in non ideal situations.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;le = LabelEncoder()
data[&#39;Activity&#39;] = le.fit_transform(data.Activity)
X = data.iloc[:,:-1]    
y = data.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)
classifier = SVC()
clf = classifier.fit(X_train, y_train)
y_pred = clf.predict(X_test)
precision, recall, f_score, _ = error_metric(y_test, y_pred, average = &#39;weighted&#39;)
model1_score = accuracy_score(y_test, y_pred)
print(model1_score)
print(precision, recall, f_score)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/6.jpg&#34; alt=&#34;plot6&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;22-feature-selection-and-removing-top-correlated-variables&#34;&gt;2.2 Feature selection and removing top correlated variables&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have near about 500+ features in our dataset. Not every feature contributes towards the model prediction hence we can cut down the no. of features according to their importance and then start a model building on top of it. This vague idea or process is called feature selection. We will be training a new scv model on new reduced dataset. We are reducing the features with the help of sklearn SelectFromModel. Lets see the accuracy metrics of this new trained model and shape of our new dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lsvc = LinearSVC(C = 0.01, penalty=&amp;quot;l1&amp;quot;, dual=False, random_state=42).fit(X_train, y_train)
model_2 = SelectFromModel(lsvc, prefit=True)
new_train_features = model_2.transform(X_train)
new_test_features = model_2.transform(X_test)
print(new_train_features.shape,new_test_features.shape )
classifier_2 = SVC()
clf_2 = classifier_2.fit(new_train_features, y_train)
y_pred_new = clf_2.predict(new_test_features)
model2_score =accuracy_score(y_test, y_pred_new)
precision, recall, f_score, _ = error_metric(y_test, y_pred_new, average=&#39;weighted&#39;)
print(model2_score)
print(precision, recall, f_score)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/7.jpg&#34; alt=&#34;plot7&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As we can see that our model is still working really well even after reduction of so many features. We have reduced features from 562 to 110 and you can compare the accuracy from previous model it is almost equivalent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;4-hyperparameter-tuning-for-svc-with-grid-search-and-final-model-building&#34;&gt;4. Hyperparameter Tuning for SVC with Grid Search and final model building.&lt;/h2&gt;
&lt;p&gt;Model hyperparmeters are configurations that is external to the model and whose value cannot be estimated from data. We will use gridsearch here with both rbf and linear kernel and value of C from 0.1 to 100.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;parameters = {
    &#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;],
    &#39;C&#39;: [100, 20, 1, 0.1]
}

selector = GridSearchCV(SVC(), parameters, scoring=&#39;accuracy&#39;) 
selector.fit(new_train_features, y_train)

print(&#39;Best parameter set found:&#39;)
print(selector.best_params_)
print(&#39;Detailed grid scores:&#39;)
means = selector.cv_results_[&#39;mean_test_score&#39;]
stds = selector.cv_results_[&#39;std_test_score&#39;]
for mean, std, params in zip(means, stds, selector.cv_results_[&#39;params&#39;]):
    print(&#39;%0.3f (+/-%0.03f) for %r&#39; % (mean, std * 2, params))
    print()

classifier_3 = SVC(kernel=&#39;rbf&#39;, C=100)
clf_3 = classifier_3.fit(new_train_features, y_train)
y_pred_final = clf_3.predict(new_test_features)
model3_score = accuracy_score(y_test, y_pred_final)

print(&#39;Accuracy score:&#39;, model3_score)

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/8.jpg&#34; alt=&#34;plot8&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As we can see this is the best hyperparameter {&amp;lsquo;C&amp;rsquo;: 100, &amp;lsquo;kernel&amp;rsquo;: &amp;lsquo;rbf&amp;rsquo;} with accuracy of 98.38%&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-and-data&#34;&gt;Code and Data&lt;/h2&gt;
&lt;p&gt;Full Code and Dataset can be found &lt;a class=&#34;link&#34; href=&#34;https://github.com/shubendu/human-activity-recognition&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/h2&gt;
&lt;p&gt;Congratulations if you have reached her :) We can try tree based models such as random forest and GBDT. If You have any question regarding this blog feel free to contact me on my website&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://greyatom.com/&#34;&gt;https://greyatom.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.appliedaicourse.com/&#34;&gt;https://www.appliedaicourse.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Financial Distress Prediction using SMOTE</title>
        <link>https://shubendu.github.io/p/financial-distress-prediction-using-smote/</link>
        <pubDate>Sat, 11 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/financial-distress-prediction-using-smote/</guid>
        <description>&lt;h1 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h1&gt;
&lt;p&gt;Credit scoring algorithms, which makes a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. We will predicting the probability that somebody will experience financial distress in the next two years.&lt;/p&gt;
&lt;h1 id=&#34;dataset&#34;&gt;Dataset&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SeriousDlqin2yrs&lt;/td&gt;
&lt;td&gt;Person experienced 90 days past due delinquency or worse&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RevolvingUtilizationOfUnsecuredLines&lt;/td&gt;
&lt;td&gt;Total balance on credit cards and personal lines of credit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td&gt;Age of borrower in years&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberOfTime30-59DaysPastDueNotWorse&lt;/td&gt;
&lt;td&gt;Number of times borrower has been 30-59 days past due but no worse in the last 2 years&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DebtRatio&lt;/td&gt;
&lt;td&gt;Monthly debt payments, alimony,living costs divided by monthy gross income&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MonthlyIncome&lt;/td&gt;
&lt;td&gt;Monthly Income&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberOfOpenCreditLinesAndLoans&lt;/td&gt;
&lt;td&gt;Number of Open loans (installment like car loan or mortgage) and Lines of credit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberOfTimes90DaysLate&lt;/td&gt;
&lt;td&gt;Number of times borrower has been 90 days or more past due&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberRealEstateLoansOrLines&lt;/td&gt;
&lt;td&gt;Number of mortgage and real estate loans including home equity lines of credit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberOfTime60-89DaysPastDueNotWorse&lt;/td&gt;
&lt;td&gt;Number of times borrower has been 60-89 days past due but no worse in the last 2 years&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberOfDependents&lt;/td&gt;
&lt;td&gt;Number of dependents in family excluding themselves&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;importing-libraries&#34;&gt;Importing libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;1-eda&#34;&gt;1. EDA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;11-loading-dataset-and-splitting-into-train-test-sets&#34;&gt;1.1 Loading Dataset and splitting into train test sets&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First we&amp;rsquo;ll load the dataset with pandas and split it into X and y variable where X contains our features and y contains our target variable. This can be done by the following code&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = pd.read_csv(&#39;financial.csv&#39;).drop(&#39;Unnamed: 0&#39;,1)
X = df.drop([&#39;SeriousDlqin2yrs&#39;],axis = 1)
y = df[&#39;SeriousDlqin2yrs&#39;]
count = y.value_counts()
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 6)
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;12-visualising-the-data&#34;&gt;1.2 Visualising the data&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Visualization is an important part as you can get an idea of the data just by looking at different graphs. We will try to get an idea of how our features are related to the dependent variable and get some insights about the data using scatter plot. The below code will do that.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cols = list(X.columns)
fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10,25))
for i in range(0,5):
    for j in range(0,2):
        col= cols[i * 2 + j]
        axes[i,j].set_title(col)
        axes[i,j].scatter(X_train[col],y_train)
        axes[i,j].set_xlabel(col)
        axes[i,j].set_ylabel(&#39;SeriousDlqin2yrs&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/1.jpg&#34; alt=&#34;scatter plot&#34;  /&gt;
&lt;img src=&#34;https://shubendu.github.io/img/financial/2.jpg&#34; alt=&#34;scatter plot2&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;13-value-count-of-classes&#34;&gt;1.3 Value count of classes&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we can see our dataset is highly imbalanced&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sns.countplot(df[&#39;SeriousDlqin2yrs&#39;])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/imbalance.jpg&#34; alt=&#34;value counts&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-data-preprocessing&#34;&gt;2. Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;21-checking-for-missing-values-and-replace-it-with-appropriate-values&#34;&gt;2.1 Checking for missing values and replace it with appropriate values&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Real-world data often has missing values. Data can have missing values for a number of reasons such as observations that were not recorded and data corruption. Handling missing data is important as many machine learning algorithms do not support data with missing values. We will check for missing values and replace them with appropriate values.The below code will do the task.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(X_train.isnull().sum())
X_train[&#39;MonthlyIncome&#39;].fillna(X_train[&#39;MonthlyIncome&#39;].median(),inplace = True)
X_train[&#39;NumberOfDependents&#39;].fillna(X_train[&#39;NumberOfDependents&#39;].median(),inplace = True)
X_test[&#39;MonthlyIncome&#39;].fillna(X_test[&#39;MonthlyIncome&#39;].median(),inplace = True)
X_test[&#39;NumberOfDependents&#39;].fillna(X_test[&#39;NumberOfDependents&#39;].median(),inplace = True)
print(X_test.isnull().sum())
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/fillna.jpg&#34; alt=&#34;fillna&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;22-feature-selection&#34;&gt;2.2 Feature selection&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We always try to reduce our features to the minimum number of most significant features. The basic rule of feature selection is that we need to select features which are highly correlated to the dependent variable and also not highly correlated with each other as they show the same trend. We do this with the help of the correlation matrix. So, we will find the features which are highly correlated and select the most significant features amongst them.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;corr = X_train.corr()
plt.figure(figsize=(14,12))
sns.heatmap(corr, annot=True, fmt=&amp;quot;.2g&amp;quot;)
X_train.drop([&#39;NumberOfTime30-59DaysPastDueNotWorse&#39;,&#39;NumberOfTime60-89DaysPastDueNotWorse&#39;],axis = 1,inplace=True)
X_test.drop([&#39;NumberOfTime30-59DaysPastDueNotWorse&#39;,&#39;NumberOfTime60-89DaysPastDueNotWorse&#39;],axis = 1,inplace=True)

print(X_train.columns)
print(X_test.columns)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/corr.jpg&#34; alt=&#34;heatmap&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As we can see from the heat map that the features NumberOfTime30-59DaysPastDueNotWorse, NumberOfTimes90DaysLate and NumberOfTime60-89DaysPastDueNotWorse are highly correlated. So we are dropping NumberOfTime30-59DaysPastDueNotWorse and NumberOfTime60-89DaysPastDueNotWorse from X_train and X_test as well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;23-scaling-the-features&#34;&gt;2.3 Scaling the features&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While working with the learning model, it is important to scale the features to a range which is centered around zero so that the variance of the features are in the same range. If the feature’s variance is orders of magnitude more than the variance of other features, that particular feature might dominate other features in the dataset and our model will not train well which gives us bad model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3-model&#34;&gt;3. Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;31-predict-the-values-after-building-a-machine-learning-model&#34;&gt;3.1 Predict the values after building a Machine learning model&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Logistic regression is another technique borrowed by machine learning from the field of statistics.It is the go-to method for binary classification problems (problems with two class values). In this post we will discover the logistic regression algorithm for machine learning.This is a classification problem to predict whether somebody will face financial distress in the next two years. So, here we will train our data on a Logistic regression algorithm and try to correctly predict the class.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;log_reg = LogisticRegression()
log_reg.fit(X_train,y_train)
y_pred = log_reg.predict(X_test)
accuracy = accuracy_score(y_test,y_pred)
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;32-is-our-prediction-right&#34;&gt;3.2 Is our prediction right?&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simply, building a predictive model is not your motive. But, creating and selecting a model which gives high accuracy. Hence, it is crucial to check accuracy of the model and we consider different kinds of metrics to evaluate our models. The choice of metric completely depends on the type of model and the implementation plan of the model. After you are finished building your model, these metrics will help you in evaluating your model accuracy.We will print the classification report of our model. This can be done by the following code&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;score = roc_auc_score(y_pred , y_test)
y_pred_proba = log_reg.predict_proba(X_test)[:,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label=&amp;quot;Logistic model, auc=&amp;quot;+str(auc))
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.legend(loc=4)
plt.show()
f1 = f1_score(y_test, log_reg.predict(X_test))
precision = precision_score(y_test, log_reg.predict(X_test))
recall = recall_score(y_test, log_reg.predict(X_test))
roc_auc = roc_auc_score(y_test, log_reg.predict(X_test))
print (&#39;Confusion_matrix&#39; + &#39;\n&#39;, confusion_matrix(y_test, log_reg.predict(X_test)))
print (&#39;Classification_report&#39; + &#39;\n&#39; + classification_report(y_test,y_pred))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/classification.jpg&#34; alt=&#34;classification&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As we can see our model is not working well in calss 1 category because this is an imbalanced data we will now try to fix this. Auc metric helps us to determine if our model is working well in imabalnced class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;4-smote&#34;&gt;4. SMOTE&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;41-balancing-the-dataset-using-smote&#34;&gt;4.1 Balancing the dataset using SMOTE&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we can see that the dataset is not balanced and it shows that 93% of customers will not face financial distress.In this situation, the predictive model developed using conventional machine learning algorithms could be biased and inaccurate.This happens because Machine Learning Algorithms are usually designed to improve accuracy by reducing the error. Thus, they do not take into account the class distribution / proportion or balance of classes. If we train our model on such an imbalanced data we will get incorrect predictions. To overcome this there are different methods such undersampling, oversampling and SMOTE. We will be using the SMOTE technique.Check for different evaluation metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;count = y.value_counts()
smote = SMOTE(random_state=9)
X_sample, y_sample = smote.fit_sample(X_train, y_train)
sns.countplot(y_sample)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/5.jpg&#34; alt=&#34;smote&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As we can see now our dataset is balanced now. Lets try applying new model on this dataset.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;42-effect-of-applying-smote&#34;&gt;4.2 Effect of applying SMOTE?&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SMOTE is an over-sampling method. What it does is, it creates synthetic (not duplicate) samples of the minority class. Hence making the minority class equal to the majority class. After applying &amp;lsquo;SMOTE&amp;rsquo; we have balanced the data. We will use this balanced data for training our model and check whether the performance of our model has improved or not by comparing different evaluation parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;log_reg.fit(X_sample, y_sample)
y_pred = log_reg.predict(X_test)
score = roc_auc_score(y_pred , y_test)
y_pred_proba = log_reg.predict_proba(X_test)[:,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label=&amp;quot;Logistic model, auc=&amp;quot;+str(auc))
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.legend(loc=4)
plt.show()
f1 = f1_score(y_test, log_reg.predict(X_test))
precision = precision_score(y_test, log_reg.predict(X_test))
recall = recall_score(y_test, log_reg.predict(X_test))
# roc_auc = roc_auc_score(y_test, log_reg.predict(X_test))
print(&#39;Confusion matrix&#39; + &#39;\n&#39; ,confusion_matrix(y_test, log_reg.predict(X_test)))
print (&#39;Classification_report&#39; + &#39;\n&#39; + classification_report(y_test,y_pred))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/SMOTE_report.jpg&#34; alt=&#34;smote&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you compare previos classification report to this one you can observe that our model has imporved pretty much for our minority class. F1 score of class 1 has been improved from 3% to 21% , thats quite an improvemnt!! See the power of SMOTE.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;5-random-forest-algorithm&#34;&gt;5. Random Forest Algorithm&lt;/h2&gt;
&lt;p&gt;Random Forrest is a bagging technique which uses Decision Tree as the base model. The performance of our Logistic regression model has signifiacntly improved after balancing the data, lets check can we furthur improve it by using a Random Forrest model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rf = RandomForestClassifier(random_state=9)
rf.fit(X_sample, y_sample)
y_pred = rf.predict(X_test)
f1 = f1_score(y_test, rf.predict(X_test))
precison = precision_score(y_test, rf.predict(X_test))
recall = recall_score(y_test, rf.predict(X_test))
score = roc_auc_score(y_test, rf.predict(X_test))
print (&#39;Confusion_matrix&#39; + &#39;\n&#39;,confusion_matrix(y_test, rf.predict(X_test)))
print (&#39;Classification_report&#39; + &#39;\n&#39; + classification_report(y_test,y_pred))
score = roc_auc_score(y_pred , y_test)
y_pred_proba = rf.predict_proba(X_test)[:,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label=&amp;quot;Random_forest model, auc=&amp;quot;+str(auc))
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.legend(loc=4)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/random-forest.jpg&#34; alt=&#34;random forest&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As you can see our model has been improved.F1 score of class 1 increased from 21% to 33%.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-and-data&#34;&gt;Code and Data&lt;/h2&gt;
&lt;p&gt;Full Code and Dataset can be found &lt;a class=&#34;link&#34; href=&#34;https://github.com/shubendu/Financial-distress-prediction-using-SMOTE&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/h2&gt;
&lt;p&gt;Congratulations if you have reached her :) You can try to increase the f1 score of minority class by hyperparameter tuning of random forest. Also you can try using xgboost. If You have any question regarding this blog feel free to contact me on my website&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://greyatom.com/&#34;&gt;https://greyatom.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.appliedaicourse.com/&#34;&gt;https://www.appliedaicourse.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Markdown Syntax Guide</title>
        <link>https://shubendu.github.io/p/markdown-syntax-guide/</link>
        <pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/markdown-syntax-guide/</guid>
        <description>&lt;img src="https://shubendu.github.io/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash.jpg" alt="Featured image of post Markdown Syntax Guide" /&gt;&lt;p&gt;This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.&lt;/p&gt;
&lt;h2 id=&#34;headings&#34;&gt;Headings&lt;/h2&gt;
&lt;p&gt;The following HTML &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt;—&lt;code&gt;&amp;lt;h6&amp;gt;&lt;/code&gt; elements represent six levels of section headings. &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt; is the highest section level while &lt;code&gt;&amp;lt;h6&amp;gt;&lt;/code&gt; is the lowest.&lt;/p&gt;
&lt;h1 id=&#34;h1&#34;&gt;H1&lt;/h1&gt;
&lt;h2 id=&#34;h2&#34;&gt;H2&lt;/h2&gt;
&lt;h3 id=&#34;h3&#34;&gt;H3&lt;/h3&gt;
&lt;h4 id=&#34;h4&#34;&gt;H4&lt;/h4&gt;
&lt;h5 id=&#34;h5&#34;&gt;H5&lt;/h5&gt;
&lt;h6 id=&#34;h6&#34;&gt;H6&lt;/h6&gt;
&lt;h2 id=&#34;paragraph&#34;&gt;Paragraph&lt;/h2&gt;
&lt;p&gt;Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.&lt;/p&gt;
&lt;p&gt;Itatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.&lt;/p&gt;
&lt;h2 id=&#34;blockquotes&#34;&gt;Blockquotes&lt;/h2&gt;
&lt;p&gt;The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a &lt;code&gt;footer&lt;/code&gt; or &lt;code&gt;cite&lt;/code&gt; element, and optionally with in-line changes such as annotations and abbreviations.&lt;/p&gt;
&lt;h4 id=&#34;blockquote-without-attribution&#34;&gt;Blockquote without attribution&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Tiam, ad mint andaepu dandae nostion secatur sequo quae.
&lt;strong&gt;Note&lt;/strong&gt; that you can use &lt;em&gt;Markdown syntax&lt;/em&gt; within a blockquote.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;blockquote-with-attribution&#34;&gt;Blockquote with attribution&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Don&amp;rsquo;t communicate by sharing memory, share memory by communicating.&lt;!-- raw HTML omitted --&gt;
— &lt;!-- raw HTML omitted --&gt;Rob Pike&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;tables&#34;&gt;Tables&lt;/h2&gt;
&lt;p&gt;Tables aren&amp;rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Age&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bob&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Alice&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;inline-markdown-within-tables&#34;&gt;Inline Markdown within tables&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Italics&lt;/th&gt;
&lt;th&gt;Bold&lt;/th&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;italics&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;bold&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;code&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;code-blocks&#34;&gt;Code Blocks&lt;/h2&gt;
&lt;h4 id=&#34;code-block-with-backticks&#34;&gt;Code block with backticks&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-html&#34; data-lang=&#34;html&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&amp;lt;!doctype html&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;html&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;en&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;meta&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;charset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;utf-8&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;Example HTML5 Document&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;Test&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;html&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;code-block-indented-with-four-spaces&#34;&gt;Code block indented with four spaces&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!doctype html&amp;gt;
&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;
  &amp;lt;title&amp;gt;Example HTML5 Document&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
  &amp;lt;p&amp;gt;Test&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;code-block-with-hugos-internal-highlight-shortcode&#34;&gt;Code block with Hugo&amp;rsquo;s internal highlight shortcode&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-html&#34; data-lang=&#34;html&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&amp;lt;!doctype html&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;html&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;en&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;meta&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;charset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;utf-8&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;Example HTML5 Document&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;Test&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;html&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;list-types&#34;&gt;List Types&lt;/h2&gt;
&lt;h4 id=&#34;ordered-list&#34;&gt;Ordered List&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;First item&lt;/li&gt;
&lt;li&gt;Second item&lt;/li&gt;
&lt;li&gt;Third item&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;unordered-list&#34;&gt;Unordered List&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;List item&lt;/li&gt;
&lt;li&gt;Another item&lt;/li&gt;
&lt;li&gt;And another item&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;nested-list&#34;&gt;Nested list&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Fruit
&lt;ul&gt;
&lt;li&gt;Apple&lt;/li&gt;
&lt;li&gt;Orange&lt;/li&gt;
&lt;li&gt;Banana&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dairy
&lt;ul&gt;
&lt;li&gt;Milk&lt;/li&gt;
&lt;li&gt;Cheese&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;other-elements--abbr-sub-sup-kbd-mark&#34;&gt;Other Elements — abbr, sub, sup, kbd, mark&lt;/h2&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;GIF&lt;!-- raw HTML omitted --&gt; is a bitmap image format.&lt;/p&gt;
&lt;p&gt;H&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;O&lt;/p&gt;
&lt;p&gt;X&lt;!-- raw HTML omitted --&gt;n&lt;!-- raw HTML omitted --&gt; + Y&lt;!-- raw HTML omitted --&gt;n&lt;!-- raw HTML omitted --&gt; = Z&lt;!-- raw HTML omitted --&gt;n&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;Press &lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;CTRL&lt;!-- raw HTML omitted --&gt;+&lt;!-- raw HTML omitted --&gt;ALT&lt;!-- raw HTML omitted --&gt;+&lt;!-- raw HTML omitted --&gt;Delete&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt; to end the session.&lt;/p&gt;
&lt;p&gt;Most &lt;!-- raw HTML omitted --&gt;salamanders&lt;!-- raw HTML omitted --&gt; are nocturnal, and hunt for insects, worms, and other small creatures.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The above quote is excerpted from Rob Pike&amp;rsquo;s &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=PAAkCSZUG1c&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;talk&lt;/a&gt; during Gopherfest, November 18, 2015. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
        </item>
        <item>
        <title>Rich Content</title>
        <link>https://shubendu.github.io/p/rich-content/</link>
        <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/rich-content/</guid>
        <description>&lt;p&gt;Hugo ships with several &lt;a class=&#34;link&#34; href=&#34;https://gohugo.io/content-management/shortcodes/#use-hugo-s-built-in-shortcodes&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Built-in Shortcodes&lt;/a&gt; for rich content, along with a &lt;a class=&#34;link&#34; href=&#34;https://gohugo.io/about/hugo-and-gdpr/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Privacy Config&lt;/a&gt; and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;youtube-privacy-enhanced-shortcode&#34;&gt;YouTube Privacy Enhanced Shortcode&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe loading=&#34;lazy&#34; src=&#34;https://www.youtube.com/embed/ZJthWmvUzzc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;!-- raw HTML omitted --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;twitter-simple-shortcode&#34;&gt;Twitter Simple Shortcode&lt;/h2&gt;




&lt;style type=&#34;text/css&#34;&gt;
  .twitter-tweet {
  font: 14px/1.45 -apple-system,BlinkMacSystemFont,&#34;Segoe UI&#34;,Roboto,Oxygen-Sans,Ubuntu,Cantarell,&#34;Helvetica Neue&#34;,sans-serif;
  border-left: 4px solid #2b7bb9;
  padding-left: 1.5em;
  color: #555;
}
  .twitter-tweet a {
  color: #2b7bb9;
  text-decoration: none;
}
  blockquote.twitter-tweet a:hover,
  blockquote.twitter-tweet a:focus {
  text-decoration: underline;
}
&lt;/style&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;“In addition to being more logical, asymmetry has the advantage that its complete appearance is far more optically effective than symmetry.”&lt;br&gt;— Jan Tschichold &lt;a href=&#34;https://t.co/gcv7SrhvJb&#34;&gt;pic.twitter.com/gcv7SrhvJb&lt;/a&gt;&lt;/p&gt;&amp;mdash; Graphic Design History (@DesignReviewed) &lt;a href=&#34;https://twitter.com/DesignReviewed/status/1085870671291310081?ref_src=twsrc%5Etfw&#34;&gt;January 17, 2019&lt;/a&gt;&lt;/blockquote&gt;

&lt;!-- raw HTML omitted --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;vimeo-simple-shortcode&#34;&gt;Vimeo Simple Shortcode&lt;/h2&gt;






&lt;style&gt;
.__h_video {
   position: relative;
   padding-bottom: 56.23%;
   height: 0;
   overflow: hidden;
   width: 100%;
   background: #000;
}
.__h_video img {
   width: 100%;
   height: auto;
   color: #000;
}
.__h_video .play {
   height: 72px;
   width: 72px;
   left: 50%;
   top: 50%;
   margin-left: -36px;
   margin-top: -36px;
   position: absolute;
   cursor: pointer;
}
&lt;/style&gt;


&lt;div class=&#34;s_video_simple __h_video&#34;&gt;
&lt;a href=&#34;https://vimeo.com/4.8912912e&amp;#43;07&#34; rel=&#34;noopener&#34; target=&#34;_blank&#34;&gt;


&lt;img src=&#34;https://i.vimeocdn.com/video/337401969_640.jpg&#34; srcset=&#34;https://i.vimeocdn.com/video/337401969_640.jpg 1x, https://i.vimeocdn.com/video/337401969.jpg 2x&#34; alt=&#34;Sing Jan Swing - Kinetic Type&#34;&gt;
&lt;div class=&#34;play&#34;&gt;&lt;svg version=&#34;1&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 61 61&#34;&gt;&lt;circle cx=&#34;30.5&#34; cy=&#34;30.5&#34; r=&#34;30.5&#34; opacity=&#34;.8&#34; fill=&#34;#000&#34;&gt;&lt;/circle&gt;&lt;path d=&#34;M25.3 19.2c-2.1-1.2-3.8-.2-3.8 2.2v18.1c0 2.4 1.7 3.4 3.8 2.2l16.6-9.1c2.1-1.2 2.1-3.2 0-4.4l-16.6-9z&#34; fill=&#34;#fff&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/div&gt;&lt;/a&gt;&lt;/div&gt;</description>
        </item>
        <item>
        <title>Placeholder Text</title>
        <link>https://shubendu.github.io/p/placeholder-text/</link>
        <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/placeholder-text/</guid>
        <description>&lt;img src="https://shubendu.github.io/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash.jpg" alt="Featured image of post Placeholder Text" /&gt;&lt;p&gt;Lorem est tota propiore conpellat pectoribus de pectora summo.&lt;/p&gt;
&lt;p&gt;Redit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Exierant elisi ambit vivere dedere&lt;/li&gt;
&lt;li&gt;Duce pollice&lt;/li&gt;
&lt;li&gt;Eris modo&lt;/li&gt;
&lt;li&gt;Spargitque ferrea quos palude&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Comas hunc haec pietate fetum procerum dixit&lt;/li&gt;
&lt;li&gt;Post torum vates letum Tiresia&lt;/li&gt;
&lt;li&gt;Flumen querellas&lt;/li&gt;
&lt;li&gt;Arcanaque montibus omnes&lt;/li&gt;
&lt;li&gt;Quidem et&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;vagus-elidunt&#34;&gt;Vagus elidunt&lt;/h1&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Canons_of_page_construction#Van_de_Graaf_canon&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The Van de Graaf Canon&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;mane-refeci-capiebant-unda-mulcebat&#34;&gt;Mane refeci capiebant unda mulcebat&lt;/h2&gt;
&lt;p&gt;Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. &lt;strong&gt;Faces illo pepulere&lt;/strong&gt; tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.&lt;/p&gt;
&lt;p&gt;Iubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.&lt;/p&gt;
&lt;p&gt;Eurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel &lt;strong&gt;mitis temploque&lt;/strong&gt; vocatus, inque alis, &lt;em&gt;oculos nomen&lt;/em&gt; non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides &lt;strong&gt;parte&lt;/strong&gt;.&lt;/p&gt;</description>
        </item>
        <item>
        <title>Math Typesetting</title>
        <link>https://shubendu.github.io/p/math-typesetting/</link>
        <pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/math-typesetting/</guid>
        <description>&lt;p&gt;Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.&lt;/p&gt;
&lt;p&gt;In this example we will be using &lt;a class=&#34;link&#34; href=&#34;https://katex.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;KaTeX&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a partial under &lt;code&gt;/layouts/partials/math.html&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Within this partial reference the &lt;a class=&#34;link&#34; href=&#34;https://katex.org/docs/autorender.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Auto-render Extension&lt;/a&gt; or host these scripts locally.&lt;/li&gt;
&lt;li&gt;Include the partial in your templates like so:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;{{&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; or .Params.math .Site.Params.math &lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;{{&lt;/span&gt; partial &lt;span class=&#34;s2&#34;&gt;&amp;#34;math.html&amp;#34;&lt;/span&gt; . &lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;{{&lt;/span&gt; end &lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;To enable KaTex globally set the parameter &lt;code&gt;math&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; in a project&amp;rsquo;s configuration&lt;/li&gt;
&lt;li&gt;To enable KaTex on a per page basis include the parameter &lt;code&gt;math: true&lt;/code&gt; in content files&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Use the online reference of &lt;a class=&#34;link&#34; href=&#34;https://katex.org/docs/supported.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Supported TeX Functions&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;

&lt;p&gt;Block math:
$$
\varphi = 1+\frac{1} {1+\frac{1} {1+\frac{1} {1+\cdots} } }
$$&lt;/p&gt;</description>
        </item>
        <item>
        <title>Emoji Support</title>
        <link>https://shubendu.github.io/p/emoji-support/</link>
        <pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/emoji-support/</guid>
        <description>&lt;img src="https://shubendu.github.io/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash.jpg" alt="Featured image of post Emoji Support" /&gt;&lt;p&gt;Emoji can be enabled in a Hugo project in a number of ways.&lt;/p&gt;
&lt;p&gt;The &lt;a class=&#34;link&#34; href=&#34;https://gohugo.io/functions/emojify/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;code&gt;emojify&lt;/code&gt;&lt;/a&gt; function can be called directly in templates or &lt;a class=&#34;link&#34; href=&#34;https://gohugo.io/templates/shortcode-templates/#inline-shortcodes&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Inline Shortcodes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To enable emoji globally, set &lt;code&gt;enableEmoji&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; in your site&amp;rsquo;s &lt;a class=&#34;link&#34; href=&#34;https://gohugo.io/getting-started/configuration/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;configuration&lt;/a&gt; and then you can type emoji shorthand codes directly in content files; e.g.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;The &lt;a class=&#34;link&#34; href=&#34;http://www.emoji-cheat-sheet.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Emoji cheat sheet&lt;/a&gt; is a useful reference for emoji shorthand codes.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;N.B.&lt;/strong&gt; The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-html&#34; data-lang=&#34;html&#34;&gt;.emoji {
  font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        </item>
        
    </channel>
</rss>
