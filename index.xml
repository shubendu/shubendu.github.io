<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Shubendu Biswas</title>
        <link>https://shubendu.github.io/</link>
        <description>Recent content on Shubendu Biswas</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Thu, 13 Feb 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://shubendu.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Sentiment analysis of tweets</title>
        <link>https://shubendu.github.io/p/sentiment-analysis-of-tweets/</link>
        <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/sentiment-analysis-of-tweets/</guid>
        <description>&lt;img src="https://shubendu.github.io/p/sentiment-analysis-of-tweets/tweets.jpg" alt="Featured image of post Sentiment analysis of tweets" /&gt;&lt;h1 id=&#34;problem&#34;&gt;Problem&lt;/h1&gt;
&lt;p&gt;Twitter data is massive and as such analyzing twitter data is a mammoth undertaking. the cleaning and pre-processing of Twitter. Political polarization and reactions to new products are probably some of the biggest use-cases of twitter data analytics. We have dataset containing user tweets. These tweets can be negative (0) and positive (4). Based on the obtained data our goal is to identify the sentiment/polarity of the tweets.&lt;/p&gt;
&lt;h1 id=&#34;about-dataset&#34;&gt;About Dataset&lt;/h1&gt;
&lt;p&gt;There are 6 columns/features in the dataset and are described below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;target: the polarity of the tweet (0 = negative, 4 = positive)&lt;/li&gt;
&lt;li&gt;ids: The id of the tweet ( ex :2087)&lt;/li&gt;
&lt;li&gt;date: the date of the tweet (ex: Sat May 16 23:58:44 UTC 2009)&lt;/li&gt;
&lt;li&gt;flag: The query (lyx). If there is no query, then this value is NO_QUERY.&lt;/li&gt;
&lt;li&gt;user: the user-name of the user that tweeted&lt;/li&gt;
&lt;li&gt;text: the text of the tweet&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;importing-libraries&#34;&gt;Importing libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support as error_metric
from nltk.corpus import stopwords
import nltk
from string import punctuation
from nltk.stem.porter import *
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;eda&#34;&gt;EDA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;loading-dataset-checking-for-null-values-and-sampling-from-data&#34;&gt;Loading Dataset ,checking for null values and sampling from data&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first step - you know the drill by now - load the dataset and see how it looks like. In this task, we are basically looking for glimpse of overall data and look at the null values if they are present. Since the data 1600000 rows, We will be taking sample of 5000 tweets for the sake of easy computation power.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;columns = [&#39;target&#39;,&#39;ids&#39;,&#39;date&#39;,&#39;flag&#39;,&#39;user&#39;,&#39;text&#39;]
data = pd.read_csv(&#39;file.csv&#39;, encoding = &#39;latin-1&#39;, header = None)
data.columns = columns
data.shape
data = data.sample(n=5000, random_state=2)
data
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/1.jpg&#34; alt=&#34;describe&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;sentiment-analysis-simplified&#34;&gt;Sentiment Analysis simplified&lt;/h2&gt;
&lt;p&gt;In layman&amp;rsquo;s terms sentiment analysis is basically figuring out the sentiment of a particular text. In the case with our problem statement, it is about finding out if a particular tweet is positive or negative, happy or unhappy (based on a score of course).&lt;/p&gt;
&lt;p&gt;Approach towards dealing with sentiment analysis&lt;/p&gt;
&lt;p&gt;The key to sentiment analysis is finding a number which indicates a level of positivity or negativity usually a number between 0 to 1, as it is obvious that computers cannot understand emotions such as happy or sad. However, a number could substitute an emotion and the value of this number signifies the magnitude of emotion in a direction; ex: very happy is above 0.9, happy is between 0.75 and 0.9, neutral is 0.5 and so on. The main challenge is how to obtain this number correctly with the help of given data.&lt;/p&gt;
&lt;p&gt;In supervised setting you already have information as labels are already present in the training phase. First you would clean the data of unwanted substances like stopwords, punctuation marks etc. and then extract features by converting words into numbers. You can do this extraction step in various different ways like Bag-of-Words, TF-IDF with or without n-grams approach. Then you feed some sort of classifier which outputs a labels based on some internal scoring mechanism.&lt;/p&gt;
&lt;h2 id=&#34;why-use-sentiment-analysis&#34;&gt;Why use sentiment analysis?&lt;/h2&gt;
&lt;p&gt;It’s estimated that 80% of the world’s data is unstructured and not organized in a pre-defined manner. Most of this comes from text data, like emails, support tickets, chats, social media, surveys, articles, and documents. These texts are usually difficult, time-consuming and expensive to analyze, understand, and sort through.&lt;/p&gt;
&lt;p&gt;Sentiment analysis systems allows companies to make sense of this sea of unstructured text by automating business processes, getting actionable insights, and saving hours of manual data processing, in other words, by making teams more efficient.&lt;/p&gt;
&lt;p&gt;With the help of sentiment analysis, unstructured information could be automatically transformed into structured data of public opinions about products, services, brands, politics, or any topic that people can express opinions about. This data can be very useful for commercial applications like marketing analysis, public relations, product reviews, net promoter scoring, product feedback, and customer service.&lt;/p&gt;
&lt;h2 id=&#34;issues-with-sentiment-analysis&#34;&gt;Issues with sentiment analysis&lt;/h2&gt;
&lt;p&gt;Computers have a hard time carrying out sentiment analysis tasks. Some pressing issues with performing sentiment analysis are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sarcasm: It is one of the most difficult sentiments to interpret properly. Example: &amp;ldquo;It was awesome for the week that it worked.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Relative sentiment: It is not a classic negative, but can be a negative nonetheless. Example: &amp;ldquo;I bought an iPhone&amp;rdquo; is good for Apple, but not for other mobile companies.&lt;/li&gt;
&lt;li&gt;Compound or multidimensional sentiment: These types contain positives and negatives in the same phrase. Example: &amp;ldquo;I love Mad Men, but hate the misleading episode trailers.&amp;rdquo; The above sentence consists of two polarities, i.e., Positive as well as Negative. So how do we conclude whether the review was Positive or Negative?&lt;/li&gt;
&lt;li&gt;Use of emoticons: Heavy use of emoticons (which have sentiment values) in social media texts like that of Twitter and Facebook also makes text analysis difficult.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-preprocessing&#34;&gt;Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;tackling-user-handles&#34;&gt;Tackling user handles&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tweets can be directed to any other person with username/user handle &amp;ldquo;NAME&amp;rdquo; @NAME. Consider the tweet &amp;lsquo;@brodiejay OH IM GOING THERE! Wow Mona Vale is a real place afterall! I know it sucks Mville only does the slow train pffft &amp;lsquo;. Here, &amp;lsquo;@brodiejay&amp;rsquo; is the user handle or username of the person to whom that particular tweet was directed/referred. If you take another tweet &amp;ldquo;my baby&amp;rsquo;s growing up &amp;ldquo;, it doesn&amp;rsquo;t contain any such user handle. So, in the dataset, user handles are present, but not in all observations.&lt;/p&gt;
&lt;p&gt;From basic intuition it is clear that user handles have little to zero contribution towards sentiment formation. So, it is a good idea to remove them altogether from the data. Getting rid of user handles also helps in reduction of the term-frequency matrix that gets generated. This will directly boost up calculation speed and also performance as unnecessary features will not be generated. Below code will clean our tweets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, &#39;&#39;, input_txt)

    return input_txt  
data[&#39;clean_text&#39;] = data[&#39;text&#39;].apply(lambda row:remove_pattern(row, &amp;quot;@[\w]*&amp;quot;))

data.head(5)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/2.jpg&#34; alt=&#34;clean-tweet&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;text-preprocessing&#34;&gt;Text Preprocessing&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This data can now be further preprocessed of unwanted entries which will make the data more suitable for carrying out analysis. Below discussed in brief are the preprocessing techniques that you will be carrying out at the end of this topic.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tokenization- Tokenization describes splitting paragraphs into sentences, or sentences into individual pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens. Most of what we are going to do with language relies on ﬁrst separating out such tokens separately so that subsequent preprocessing steps can be successfully done.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stopwords removal- Stopwords are the most common words in a language like &amp;lsquo;the&amp;rsquo;, &amp;lsquo;a&amp;rsquo;, &amp;lsquo;on&amp;rsquo;, &amp;lsquo;is&amp;rsquo;, &amp;lsquo;all&amp;rsquo;. These words do not carry important meaning and so are usually removed from texts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stemming- Much of natural language machine learning is about sentiment of the text. Stemming is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look). The main two algorithms are Porter stemming algorithm (removes common morphological and inflexional endings from words and Lancaster stemming algorithm (a more aggressive stemming algorithm).
Below Code will tokenize our clean text then removing stopwords from nltk, after that we will aply stemming.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;stop_words = list(set(stopwords.words(&#39;english&#39;)))+list(punctuation)+[&#39;``&#39;, &amp;quot;&#39;s&amp;quot;, &amp;quot;...&amp;quot;, &amp;quot;n&#39;t&amp;quot;]
data[&#39;tokenized_text&#39;] = [nltk.word_tokenize(x) for x in data[&#39;clean_text&#39;]]
data[&#39;tokenized_text&#39;] = data[&#39;tokenized_text&#39;].apply(lambda row: [word for word in row if word not in stop_words])
stemmer = PorterStemmer()
data[&#39;tokenized_text&#39;] = data[&#39;tokenized_text&#39;].apply(lambda x: [stemmer.stem(i) for i in x])
data[&#39;tokenized_text&#39;] = data[&#39;tokenized_text&#39;].apply(lambda x: &#39; &#39;.join(x))
data.head()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/3.jpg&#34; alt=&#34;clean-tweet&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As you can see tokenized_text&amp;rsquo;s 1st row is converted into &amp;ldquo;OH IM go there&amp;rdquo; from &amp;ldquo;OH IM GOING THERE!&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;word-cloud&#34;&gt;Word Cloud&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;word-cloud-of-all-data&#34;&gt;Word cloud of all data&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is a visualisation method that displays how frequently words appear in a given body of text, by making the size of each word proportional to its frequency. All the words are then arranged in a cluster or cloud of words. Alternatively, the words can also be arranged in any format: horizontal lines, columns or within a shape.&lt;/p&gt;
&lt;p&gt;Now that you have an idea about how to use a wordcloud, lets do a simple task to generate a wordcloud to observe the most frequently occuring words.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;all_words = &#39; &#39;.join([text for text in data[&#39;tokenized_text&#39;]])
# generate wordcloud object
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)
plt.figure(figsize=(20, 12))
plt.imshow(wordcloud, interpolation=&amp;quot;bilinear&amp;quot;)
plt.axis(&#39;off&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/4.jpg&#34; alt=&#34;word-cloud&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;word-cloud-for-negative-data&#34;&gt;Word cloud for negative data&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# negative tweets
neg_words = &#39; &#39;.join([text for text in data[&#39;tokenized_text&#39;][data[&#39;target&#39;] == 0]])

# generate wordcloud object for negative tweets
neg_wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(neg_words)
plt.figure(figsize=(20, 12))
plt.imshow(neg_wordcloud, interpolation=&amp;quot;bilinear&amp;quot;)
plt.axis(&#39;off&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/5.jpg&#34; alt=&#34;word-cloud-negative&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;word-cloud-for-postive-data&#34;&gt;Word cloud for postive data&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# positive tweets
pos_words = &#39; &#39;.join([text for text in data[&#39;tokenized_text&#39;][data[&#39;target&#39;] == 4]])

# generate wordcloud object for negative tweets
pos_wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(pos_words)
plt.figure(figsize=(20, 12))
plt.imshow(pos_wordcloud, interpolation=&amp;quot;bilinear&amp;quot;)
plt.axis(&#39;off&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/6.jpg&#34; alt=&#34;word-cloud-postive&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As you can see in negative word cloud there are lots of negative words such as fuck, bad, tire etc and in postive word cloud there are many good words.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;model-building&#34;&gt;Model Building&lt;/h2&gt;
&lt;p&gt;Logistic Regression for predicting sentiments
These steps enabled us with data in proper format as well as some interesting insights about certain keywords. This cleaned form of data can now be fed to a machine learning algorithm to classify tweets into categories. Since we are already familiar with the workflow of classifying texts, we will be only outlining the steps associated with it. The steps are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Splitting into training and test sets&lt;/li&gt;
&lt;li&gt;Construct a term-document matrix (can be done by Bag of Words or TF-IDF)&lt;/li&gt;
&lt;li&gt;Fitting a classifier on training data&lt;/li&gt;
&lt;li&gt;Predicting on test data&lt;/li&gt;
&lt;li&gt;Evaluating classifier performance&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# ratio to split into training and test set
ratio = int(len(data)*0.75)

# logistic regression model
logreg = LogisticRegression(random_state=2)

# TF-IDF feature matrix
tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words=&#39;english&#39;)

# fit and transform tweets
tweets = tfidf_vectorizer.fit_transform(data[&#39;tokenized_text&#39;])

# convert positive tweets to 1&#39;s
data[&#39;target&#39;] = data[&#39;target&#39;].apply(lambda x: 1 if x==4 else x)

# split into train and test
X_train = tweets[:ratio,:]
X_test = tweets[ratio:,:]
y_train = data[&#39;target&#39;].iloc[:ratio]
y_test = data[&#39;target&#39;].iloc[ratio:]

# fit on training data
logreg.fit(X_train,y_train)

# make predictions
prediction = logreg.predict_proba(X_test)
prediction_int = prediction[:,1] &amp;gt;= 0.3
prediction_int = prediction_int.astype(np.int)
report = classification_report(y_test,prediction_int)

# print out classification_report
print(report)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/7.jpg&#34; alt=&#34;report&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;textblob&#34;&gt;TextBlob&lt;/h2&gt;
&lt;p&gt;There are numerous libraries out there for dealing with text data. One of them is TextBlob which is built on the shoulders of NLTK and Pattern. A big advantage of TextBlob is it is easy to learn and offers a lot of features like sentiment analysis, pos-tagging, noun phrase extraction, etc. It has now become a go-to library for many users who are performing NLP tasks.&lt;/p&gt;
&lt;p&gt;Features of TextBlob&lt;/p&gt;
&lt;p&gt;The documentation page of TextBlob says; TextBlob aims to provide access to common text-processing operations through a familiar interface. You can treat TextBlob objects as if they were Python strings that learned how to do Natural Language Processing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;sentiment-analysis-with-textblob&#34;&gt;Sentiment Analysis with TextBlob&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can carry out sentiment analysis with textBlob too. The TextBlob object has an attribute sentiment that returns a tuple of the form Sentiment (polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0] with negative values corresponding to negative sentiments and positive values to positive sentiments. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.&lt;/p&gt;
&lt;p&gt;But how it does this prediction of sentiment scoring? Well, it has a training set with preclassified movie reviews, so when you give a new text for analysis, it uses NaiveBayes classifier to classify the new text&amp;rsquo;s polarity in pos and neg probabilities.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# list to store polarities
tb_polarity = []

# loop over tweets
for sentence in data[&#39;tokenized_text&#39;]:
    temp = TextBlob(sentence)
    tb_polarity.append(temp.sentiment[0])

# new column to store polarity    
data[&#39;tb_polarity&#39;] = tb_polarity
data.head(10)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/8.jpg&#34; alt=&#34;TextBlob&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can compare the target values with tb_polarity and see how much TextBlob is accurate&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;vadersentiment&#34;&gt;VaderSentiment&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/9.jpg&#34; alt=&#34;vader&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Another library for out of the box sentiment analysis is vaderSentiment. It is an open sourced python library where VADER stands for Valence Aware Dictionary and sEntiment Reasoner. With VADER you can be up and running performing sentiment classification very quickly even if you don&amp;rsquo;t have positive and negative text examples to train a classifier or want to write custom code to search for words in a sentiment lexicon. VADER is also computationally efficient when compared to other Machine Learning and Deep Learning approaches.&lt;/p&gt;
&lt;p&gt;VADER performs well on text originating in social media and is described fully in a paper entitled VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. published at ICWSM-14. VADER is able to include sentiment from emoticons (e.g, :-)), sentiment-related acronyms (e.g, LOL) and slang (e.g, meh). The developers of VADER have used Amazon’s Mechanical Turk to get most of their ratings, You can find complete details on their Github Page.&lt;/p&gt;
&lt;h3 id=&#34;output-of-vader&#34;&gt;Output of VADER&lt;/h3&gt;
&lt;p&gt;VADER produces four sentiment metrics from these word ratings. The first three metrics; positive, neutral and negative, represent the proportion of the text that falls into those categories. The final metric, the compound score, is the sum of all of the lexicon ratings which have been standardised to range between -1 and 1.&lt;/p&gt;
&lt;h3 id=&#34;how-does-vader-work&#34;&gt;How does VADER work?&lt;/h3&gt;
&lt;p&gt;First step is to import SentimentIntensityAnalyzer from vaderSentiment.vaderSentiment. Then initialize an object of SentimentIntensityAnalyzer() and use its .polarity_scores() on a given text to find about its four metrics. Below given is a code snippet:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;analyser = SentimentIntensityAnalyzer()

# empty list to store VADER polarities
vs_polarity = []

# loop over tweets
for sentence in data[&#39;tokenized_text&#39;]:
    vs_polarity.append(analyser.polarity_scores(sentence)[&#39;compound&#39;])

# add new column `&#39;vs_polarity&#39;` to data
data[&#39;vs_polarity&#39;] = vs_polarity

data.head(10)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/tweets/10.jpg&#34; alt=&#34;vader1&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;you can also compare the vs_polarity to the target variables.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-and-data&#34;&gt;Code and Data&lt;/h2&gt;
&lt;p&gt;Full Code and Dataset can be found &lt;a class=&#34;link&#34; href=&#34;https://github.com/shubendu/Twitter-Analysis&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/h2&gt;
&lt;p&gt;Congratulations if you have reached her :) We can try tree based models such as random forest and GBDT and other types of feature transformation such as word2vec. If You have any question regarding this blog feel free to contact me on my website&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://greyatom.com/&#34;&gt;https://greyatom.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Loan Defaulters</title>
        <link>https://shubendu.github.io/p/test-yolo/</link>
        <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/test-yolo/</guid>
        <description>&lt;img src="https://shubendu.github.io/p/test-yolo/loan.jpg" alt="Featured image of post Loan Defaulters" /&gt;&lt;h1 id=&#34;problem&#34;&gt;Problem&lt;/h1&gt;
&lt;p&gt;For this project we will be exploring the publicly available data from LendingClub.com. Lending Club connects people who need money (borrowers) with people who have money (investors). As an investor one would want to invest in people who showed a profile of having a high probability of paying the amount back. Using Decision Tree model, classify whether or not the borrower paid back their loan in full.&lt;/p&gt;
&lt;h1 id=&#34;about-dataset&#34;&gt;About Dataset&lt;/h1&gt;
&lt;p&gt;The snapshot of the data we will be working on:&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 302; flex-basis: 725px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/1.jpg&#34; data-size=&#34;1281x424&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/1.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/1_hu3007e5356ee912007ba1a37f74cd47c4_187321_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/1_hu3007e5356ee912007ba1a37f74cd47c4_187321_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1281&#34;
				height=&#34;424&#34;
				loading=&#34;lazy&#34;
				alt=&#34;1&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;1&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;customer.id&lt;/td&gt;
&lt;td&gt;ID of the customer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;credit.policy&lt;/td&gt;
&lt;td&gt;If the customer meets the credit underwriting criteria of LendingClub.com or not&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;purpose&lt;/td&gt;
&lt;td&gt;The purpose of the loan(takes values :&amp;ldquo;creditcard&amp;rdquo;, &amp;ldquo;debtconsolidation&amp;rdquo;, &amp;ldquo;educational&amp;rdquo;, &amp;ldquo;majorpurchase&amp;rdquo;, &amp;ldquo;smallbusiness&amp;rdquo;, and &amp;ldquo;all_other&amp;rdquo;).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;int.rate&lt;/td&gt;
&lt;td&gt;The interest rate of the loan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;installment&lt;/td&gt;
&lt;td&gt;The monthly installments owed by the borrower if the loan is funded&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;log.annual.inc&lt;/td&gt;
&lt;td&gt;The natural log of the self-reported annual income of the borrower&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dti&lt;/td&gt;
&lt;td&gt;The debt-to-income ratio of the borrower (amount of debt divided by annual income)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fico&lt;/td&gt;
&lt;td&gt;The FICO credit score of the borrower&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;days.with.cr.line&lt;/td&gt;
&lt;td&gt;The number of days the borrower has had a credit line.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;revol.bal&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s revolving balance (amount unpaid at the end of the credit card billing cycle)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;revol.util&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s revolving line utilization rate (the amount of the credit line used relative to total credit available)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pub.rec&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s number of derogatory public records (bankruptcy filings, tax liens, or judgments)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;inq.last.6mths&lt;/td&gt;
&lt;td&gt;The borrower&amp;rsquo;s number of inquiries by creditors in the last 6 months&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;delinq.2yrs&lt;/td&gt;
&lt;td&gt;The number of times the borrower had been 30+ days past due on a payment in the past 2 years&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;paid.back.loan&lt;/td&gt;
&lt;td&gt;Whether the user has paid back loan&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;importing-libraries&#34;&gt;Importing libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;loading-dataset-checking-for-null-values&#34;&gt;Loading Dataset ,checking for null values&lt;/h2&gt;
&lt;p&gt;The first step - you know the drill by now - load the dataset and see how it looks like. In this task, we are basically looking for glimpse of overall data and look at the null values if they are present also some statistical representation of our data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data  = pd.read_csv(&#39;loan.csv&#39;)
data.head().T
data.describe()
data.info()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;figure style=&#34;flex-grow: 140; flex-basis: 336px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/2.jpg&#34; data-size=&#34;606x432&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/2.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/2_hu9d7757898fcbf6997165c45944c036f6_18580_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/2_hu9d7757898fcbf6997165c45944c036f6_18580_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;606&#34;
				height=&#34;432&#34;
				loading=&#34;lazy&#34;
				alt=&#34;2&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;2&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 323; flex-basis: 776px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/3.jpg&#34; data-size=&#34;760x235&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/3.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/3_hue490a5f107723ef9886a491120812063_19658_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/3_hue490a5f107723ef9886a491120812063_19658_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;760&#34;
				height=&#34;235&#34;
				loading=&#34;lazy&#34;
				alt=&#34;3&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;3&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 98; flex-basis: 236px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/4.jpg&#34; data-size=&#34;377x383&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/4.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/4_hu56abdf877d994247b339b1b5d0da5870_21864_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/4_hu56abdf877d994247b339b1b5d0da5870_21864_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;377&#34;
				height=&#34;383&#34;
				loading=&#34;lazy&#34;
				alt=&#34;4&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;4&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Observation&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have no null values. We&amp;rsquo;ll drop the customer id, as it is of no use for our model and we have both numeric and categorical types of data which we will further preprocess.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;splitting-the-data&#34;&gt;Splitting the data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s split the data into train and test&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;X = data.drop([&#39;customer.id&#39;,&#39;paid.back.loan&#39;], axis = 1)
y = data[&#39;paid.back.loan&#39;]
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state = 0)
print(X_train.shape , y_train.shape)
print(X_test.shape, y_test.shape)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;figure style=&#34;flex-grow: 350; flex-basis: 840px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/5.jpg&#34; data-size=&#34;161x46&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/5.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/5_huebd1947e318c74369f34b7347b48e245_1515_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/5_huebd1947e318c74369f34b7347b48e245_1515_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;161&#34;
				height=&#34;46&#34;
				loading=&#34;lazy&#34;
				alt=&#34;5&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;5&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;target-variable-distribution&#34;&gt;Target variable distribution&lt;/h2&gt;
&lt;p&gt;The distribution of &amp;ldquo;paid.back.loan&amp;rdquo; and plotting barplot.&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 160; flex-basis: 384px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/6.jpg&#34; data-size=&#34;400x250&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/6.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/6_hude5eaf4f3db6a70e4e5f1e7e3361fd71_4947_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/6_hude5eaf4f3db6a70e4e5f1e7e3361fd71_4947_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;400&#34;
				height=&#34;250&#34;
				loading=&#34;lazy&#34;
				alt=&#34;6&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;6&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can see that 5639 people have paid back loan while 1065 people not paid back the loan.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;feature-enginnering&#34;&gt;Feature Enginnering&lt;/h2&gt;
&lt;p&gt;We need to preprocess data beofre feature engineering as we can see that &amp;ldquo;int.rate&amp;rdquo; column has percentage symbol which need to be remove and later I am dividing that column with 100 to get the actual percentage values. After that I will be seperating the data into numeric and categorical dataframe.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Removing the last character from the values in column
X_train[&#39;int.rate&#39;] = X_train[&#39;int.rate&#39;].map(lambda x: str(x)[:-1])

#Dividing the column values by 100
X_train[&#39;int.rate&#39;]=X_train[&#39;int.rate&#39;].astype(float)/100

#Removing the last character from the values in column
X_test[&#39;int.rate&#39;] = X_test[&#39;int.rate&#39;].map(lambda x: str(x)[:-1])

#Dividing the column values by 100
X_test[&#39;int.rate&#39;]=X_test[&#39;int.rate&#39;].astype(float)/100

#Storing all the numerical type columns in &#39;num_df&#39;
num_df=X_train.select_dtypes(include=[&#39;number&#39;]).copy()

#Storing all the categorical type columns in &#39;cat_df&#39;
cat_df=X_train.select_dtypes(include=[&#39;object&#39;]).copy()
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;feature-visualisation&#34;&gt;Feature Visualisation&lt;/h2&gt;
&lt;p&gt;Now we can visualise the distribuiton of our numeric dataset in different calss variable. Below code will do the job&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cols=list(num_df.columns)
for i in range(9):          
    
    #Plotting boxplot
    sns.boxplot(x=y_train,y=num_df[cols[i]],ax=axes[i])
    
    #Avoiding subplots overlapping
    fig.tight_layout()    
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;figure style=&#34;flex-grow: 120; flex-basis: 288px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/7.jpg&#34; data-size=&#34;977x814&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/7.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/7_hu919fb089814596a722565968106ab488_38290_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/7_hu919fb089814596a722565968106ab488_38290_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;977&#34;
				height=&#34;814&#34;
				loading=&#34;lazy&#34;
				alt=&#34;7&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;7&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure style=&#34;flex-grow: 118; flex-basis: 285px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/8.jpg&#34; data-size=&#34;980x824&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/8.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/8_hu2cc328802c0a3f2b9b6e3f749365f05a_41758_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/8_hu2cc328802c0a3f2b9b6e3f749365f05a_41758_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;980&#34;
				height=&#34;824&#34;
				loading=&#34;lazy&#34;
				alt=&#34;8&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;8&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Most of our features has different distribution for our class variable which is good for our model&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;Lets&amp;rsquo;s visualise the categorical features as well. I will be plotting using seaborn to see how our distribution differs in different class.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cols=list(cat_df.columns)
#Looping through rows
for i in range(0,2):
    
    #Looping through columns
    for j in range(0,2):
        
        #Plotting count plot
        sns.countplot(x=X_train[cols[i*2+j]], hue=y_train,ax=axes[i,j])                        
        
        #Avoiding subplots overlapping
        fig.tight_layout()    

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;figure style=&#34;flex-grow: 202; flex-basis: 486px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/9.jpg&#34; data-size=&#34;1329x656&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/9.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/9_hudb0518cdb1fd8378a043fa4b07db9d1b_31110_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/9_hudb0518cdb1fd8378a043fa4b07db9d1b_31110_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1329&#34;
				height=&#34;656&#34;
				loading=&#34;lazy&#34;
				alt=&#34;9&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;9&lt;/figcaption&gt;
		
	&lt;/figure&gt;
&lt;figure style=&#34;flex-grow: 197; flex-basis: 474px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/10.jpg&#34; data-size=&#34;1323x669&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/10.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/10_hu70abdca00357f4d4fd44567ef6434d21_27027_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/10_hu70abdca00357f4d4fd44567ef6434d21_27027_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1323&#34;
				height=&#34;669&#34;
				loading=&#34;lazy&#34;
				alt=&#34;10&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;10&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can see that the major reason that stands common for the majority of customers who have applied for a loan is debt_consolidation which means taking one loan to payoff there other loans.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;1&lt;/p&gt;
&lt;h2 id=&#34;model-building&#34;&gt;Model Building&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s Apply the Decision Tree classifier to our dataset. We will encode the categorical features using label encoder.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for col in cat_df.columns:
    
    #Filling null values with &#39;NA&#39;
    X_train[col].fillna(&#39;NA&#39;,inplace=True)
    
    #Initalising a label encoder object
    le=LabelEncoder()
    
    #Fitting and transforming the column in X_train with &#39;le&#39;
    X_train[col]=le.fit_transform(X_train[col]) 
    
    #Filling null values with &#39;NA&#39;
    X_test[col].fillna(&#39;NA&#39;,inplace=True)
    
    #Fitting the column in X_test with &#39;le&#39;
    X_test[col]=le.transform(X_test[col]) 

# Replacing the values of y_train
y_train.replace({&#39;No&#39;:0,&#39;Yes&#39;:1},inplace=True)

# Replacing the values of y_test
y_test.replace({&#39;No&#39;:0,&#39;Yes&#39;:1},inplace=True)

#Initialising &#39;Decision Tree&#39; model    
model=DecisionTreeClassifier(random_state=0)

#Training the &#39;Decision Tree&#39; model
model.fit(X_train, y_train)

#Finding the accuracy of &#39;Decision Tree&#39; model
acc=model.score(X_test, y_test)

#Printing the accuracy
print(acc)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;figure style=&#34;flex-grow: 487; flex-basis: 1170px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/11.jpg&#34; data-size=&#34;156x32&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/11.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/11_hua2ba2c0e7588f7cb4a7c3c364cff4280_968_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/11_hua2ba2c0e7588f7cb4a7c3c364cff4280_968_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;156&#34;
				height=&#34;32&#34;
				loading=&#34;lazy&#34;
				alt=&#34;11&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;11&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have total 74% accuracy on our model without having any hyperparameter tuning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;decision-tree-pruning&#34;&gt;Decision Tree Pruning&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s see if pruning of decision tree improves its accuracy. We will use grid search to do the optimum pruning.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;parameter_grid = {&#39;max_depth&#39;: np.arange(3,10), &#39;min_samples_leaf&#39;: range(10,50,10)}

#Code starts here

#Initialising &#39;Decision Tree&#39; model
model_2 = DecisionTreeClassifier(random_state=0)

#Applying Grid Search of hyper-parameters and finding the optimum &#39;Decision Tree&#39; model
p_tree = GridSearchCV(model_2, parameter_grid, cv=5)

#Training the optimum &#39;Decision Tree&#39; model
p_tree.fit(X_train, y_train)

#Finding the accuracy of the optimum &#39;Decision Tree&#39; model
acc_2 = p_tree.score(X_test, y_test)

#Printing the accuracy
print(acc_2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;figure style=&#34;flex-grow: 546; flex-basis: 1310px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/12.jpg&#34; data-size=&#34;142x26&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/12.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/12_hu30cd7a09f3422447a02027b590dd2cd9_875_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/12_hu30cd7a09f3422447a02027b590dd2cd9_875_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;142&#34;
				height=&#34;26&#34;
				loading=&#34;lazy&#34;
				alt=&#34;12&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;12&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Great our accuracy has improved drastically.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;tree-visualising&#34;&gt;Tree visualising&lt;/h2&gt;
&lt;p&gt;we can also visualise our tree.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Importing header files

from io import StringIO
from sklearn.tree import export_graphviz
from sklearn import tree
from sklearn import metrics
from IPython.display import Image
import pydotplus


#Creating DOT data
dot_data = export_graphviz(decision_tree=p_tree.best_estimator_, out_file=None, 
                                feature_names=X.columns, filled = True,  
                                class_names=[&#39;loan_paid_back_yes&#39;,&#39;loan_paid_back_no&#39;])

#Drawing graph
graph_big = pydotplus.graph_from_dot_data(dot_data)  

#Displaying graph
# show graph - do not delete/modify the code below this line

img_path = user_data_dir+&#39;/file.png&#39;
graph_big.write_png(img_path)

plt.figure(figsize=(20,15))
plt.imshow(plt.imread(img_path))
plt.axis(&#39;off&#39;)
plt.show() 

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;figure style=&#34;flex-grow: 292; flex-basis: 702px&#34;&gt;
		&lt;a href=&#34;https://shubendu.github.io/p/test-yolo/13.jpg&#34; data-size=&#34;1056x361&#34;&gt;&lt;img src=&#34;https://shubendu.github.io/p/test-yolo/13.jpg&#34;
				srcset=&#34;https://shubendu.github.io/p/test-yolo/13_hu4cf3f7ea4df67abd21b4d9fdea0f6ad6_83430_480x0_resize_q75_box.jpg 480w, https://shubendu.github.io/p/test-yolo/13_hu4cf3f7ea4df67abd21b4d9fdea0f6ad6_83430_1024x0_resize_q75_box.jpg 1024w&#34;
				width=&#34;1056&#34;
				height=&#34;361&#34;
				loading=&#34;lazy&#34;
				alt=&#34;13&#34;&gt;
		&lt;/a&gt;
		
		&lt;figcaption&gt;13&lt;/figcaption&gt;
		
	&lt;/figure&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-and-data&#34;&gt;Code and Data&lt;/h2&gt;
&lt;p&gt;Full Code and Dataset can be found &lt;a class=&#34;link&#34; href=&#34;https://github.com/shubendu/Loan-Defaulters&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/h2&gt;
&lt;p&gt;Congratulations if you have reached her :) We can try GBDT and XgBoost to increase our model accuracy.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://greyatom.com/&#34;&gt;https://greyatom.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Human Activity Recognition with Smartphones</title>
        <link>https://shubendu.github.io/p/human-activity-recognition-with-smartphones/</link>
        <pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/human-activity-recognition-with-smartphones/</guid>
        <description>&lt;img src="https://shubendu.github.io/p/human-activity-recognition-with-smartphones/actt.jpg" alt="Featured image of post Human Activity Recognition with Smartphones" /&gt;&lt;h1 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h1&gt;
&lt;p&gt;The Human Activity Recognition database was built from the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. The objective is to classify activities into one of the six activities performed.&lt;/p&gt;
&lt;h1 id=&#34;about-dataset&#34;&gt;About Dataset&lt;/h1&gt;
&lt;p&gt;The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKINGUPSTAIRS, WALKINGDOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.&lt;/p&gt;
&lt;p&gt;The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.&lt;/p&gt;
&lt;h1 id=&#34;attribute-information&#34;&gt;Attribute information&lt;/h1&gt;
&lt;p&gt;For each record in the dataset the following is provided:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Triaxial Angular velocity from the gyroscope.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A 561-feature vector with time and frequency domain variables.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Its activity label.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An identifier of the subject who carried out the experiment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;importing-libraries&#34;&gt;Importing libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import precision_recall_fscore_support as error_metric
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;1-eda&#34;&gt;1. EDA&lt;/h2&gt;
&lt;h3 id=&#34;11-loading-dataset-checking-for-null-values-and-statistical-description&#34;&gt;1.1 Loading Dataset ,checking for null values and statistical description&lt;/h3&gt;
&lt;p&gt;The first step - you know the drill by now - load the dataset and see how it looks like. In this task, we are basically looking for descriptive statistics of overall data and look at the null values if they are present. Since the data is too large I will be only printiing till 25 columns.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data = pd.read_csv(&#39;file.csv&#39;)
print(&#39;Null Values In DataFrame: {}\n&#39;.format(data.isna().sum().sum()))
data.describe().T[:25]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/1.jpg&#34; alt=&#34;describe&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;We have 563 features and 10299 rows&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/2.jpg&#34; alt=&#34;describe-2&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After observing the dataset statistical description majority of the values are having a minimum value of -1 and maximum value of 1 which brings us to the conclusion that scaling is not required in this case. Also the data set is free of any kind of null values.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;12-distribution-of-our-class&#34;&gt;1.2 Distribution of our class&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s get a little bit deeper dive into exploring the target feature via graphical methods.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;label = data[&#39;Activity&#39;]
sns.countplot(x= label)
plt.xticks(rotation=75);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/3.jpg&#34; alt=&#34;class-distribution&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The class distribution are somewhat balanced which is a better thing on prediction and model building part.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;13-how-long-does-the-participant-use-the-staircase&#34;&gt;1.3 How Long Does The Participant Use The Staircase?&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since the dataset has been created in an scientific environment nearly equal preconditions for the participants can be assumed. It is highly likely for the participants to have been walking up and down the same number of staircases. Bascially we are looking to calculate how much time the each participant has been spending while walking downstairs and upstairs as it seems to be most frequent activity. Let us investigate their activity durations. The description of the data states; &amp;ldquo;fixed-width sliding windows of 2.56 sec and 50% overlap&amp;rdquo; for each datapoint.Therefore a single datapoint is gathered every 1.28 sec.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data_copy = data.copy() 
data_copy[&#39;duration&#39;] = &#39;&#39;
duration_df = (data_copy.groupby([label[label.isin([&#39;WALKING_UPSTAIRS&#39;, &#39;WALKING_DOWNSTAIRS&#39;])], &#39;subject&#39;])[&#39;duration&#39;].count() * 1.28)
duration_df = pd.DataFrame(duration_df)
plot_data = duration_df.reset_index().sort_values(&#39;duration&#39;, ascending=False)
plot_data[&#39;Activity&#39;] = plot_data[&#39;Activity&#39;].map({&#39;WALKING_UPSTAIRS&#39;:&#39;Upstairs&#39;, &#39;WALKING_DOWNSTAIRS&#39;:&#39;Downstairs&#39;})
plt.figure(figsize=(15,5))
sns.barplot(data=plot_data, x=&#39;subject&#39;, y=&#39;duration&#39;, hue=&#39;Activity&#39;)
plt.title(&#39;Participants Compared By Their Staircase Walking Duration&#39;)
plt.xlabel(&#39;Participants&#39;)
plt.ylabel(&#39;Total Duration [s]&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/4.jpg&#34; alt=&#34;plot4&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Nearly all participants have more data for walking upstairs than downstairs. Assuming an equal number of up- and down-walks the participants need longer walking upstairs. Furthermore the range of the duration is narrow and adjusted to the conditions. A young person being ~50% fast in walking upstairs than an older one is reasonable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;14-finding-the-correlation&#34;&gt;1.4 Finding the correlation&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We always try to reduce our features to the minimum number of most significant features. The basic rule of feature selection is that we need to select features which are highly correlated to the dependent variable and also not highly correlated with each other as they show the same trend. Correlation refers to the mutual relationship and association between quantities and it is generaly used to express one quantity in terms of its relationship with other quantities. This can either be Positive(variables change in the same direction), negative(variables change in opposite direction or neutral(No correlation). Let&amp;rsquo;s investigate the correlated pairs. We are storing the correlated pairs exculding self correlated pairs in the variable named as top_corr_feilds with a threshold of 0.8 for absolute correlation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
feature_cols = data.columns[: -2]   
correlated_values = data[feature_cols].corr()
correlated_values = (correlated_values.stack().to_frame().reset_index().rename(columns={&#39;level_0&#39;: &#39;Feature_1&#39;, &#39;level_1&#39;: &#39;Feature_2&#39;, 0:&#39;Correlation_score&#39;}))
correlated_values[&#39;abs_correlation&#39;] = correlated_values.Correlation_score.abs()
top_corr_fields = correlated_values.sort_values(&#39;Correlation_score&#39;, ascending = False).query(&#39;abs_correlation&amp;gt;0.8 &#39;)
top_corr_fields = top_corr_fields[top_corr_fields[&#39;Feature_1&#39;] != top_corr_fields[&#39;Feature_2&#39;]].reset_index(drop=True)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/5.jpg&#34; alt=&#34;plot5&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-data-processing-and-baseline-model&#34;&gt;2. Data Processing and baseline model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;21-data-preparation-and-implementing-baseline-model&#34;&gt;2.1 Data preparation and implementing baseline model&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, we will be building a baseline model of svm which would take data with insignificant amount of data preprocessing which is to observe how our model is performing in non ideal situations.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;le = LabelEncoder()
data[&#39;Activity&#39;] = le.fit_transform(data.Activity)
X = data.iloc[:,:-1]    
y = data.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)
classifier = SVC()
clf = classifier.fit(X_train, y_train)
y_pred = clf.predict(X_test)
precision, recall, f_score, _ = error_metric(y_test, y_pred, average = &#39;weighted&#39;)
model1_score = accuracy_score(y_test, y_pred)
print(model1_score)
print(precision, recall, f_score)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/6.jpg&#34; alt=&#34;plot6&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;22-feature-selection-and-removing-top-correlated-variables&#34;&gt;2.2 Feature selection and removing top correlated variables&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have near about 500+ features in our dataset. Not every feature contributes towards the model prediction hence we can cut down the no. of features according to their importance and then start a model building on top of it. This vague idea or process is called feature selection. We will be training a new scv model on new reduced dataset. We are reducing the features with the help of sklearn SelectFromModel. Lets see the accuracy metrics of this new trained model and shape of our new dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lsvc = LinearSVC(C = 0.01, penalty=&amp;quot;l1&amp;quot;, dual=False, random_state=42).fit(X_train, y_train)
model_2 = SelectFromModel(lsvc, prefit=True)
new_train_features = model_2.transform(X_train)
new_test_features = model_2.transform(X_test)
print(new_train_features.shape,new_test_features.shape )
classifier_2 = SVC()
clf_2 = classifier_2.fit(new_train_features, y_train)
y_pred_new = clf_2.predict(new_test_features)
model2_score =accuracy_score(y_test, y_pred_new)
precision, recall, f_score, _ = error_metric(y_test, y_pred_new, average=&#39;weighted&#39;)
print(model2_score)
print(precision, recall, f_score)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/7.jpg&#34; alt=&#34;plot7&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As we can see that our model is still working really well even after reduction of so many features. We have reduced features from 562 to 110 and you can compare the accuracy from previous model it is almost equivalent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;4-hyperparameter-tuning-for-svc-with-grid-search-and-final-model-building&#34;&gt;4. Hyperparameter Tuning for SVC with Grid Search and final model building.&lt;/h2&gt;
&lt;p&gt;Model hyperparmeters are configurations that is external to the model and whose value cannot be estimated from data. We will use gridsearch here with both rbf and linear kernel and value of C from 0.1 to 100.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;parameters = {
    &#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;],
    &#39;C&#39;: [100, 20, 1, 0.1]
}

selector = GridSearchCV(SVC(), parameters, scoring=&#39;accuracy&#39;) 
selector.fit(new_train_features, y_train)

print(&#39;Best parameter set found:&#39;)
print(selector.best_params_)
print(&#39;Detailed grid scores:&#39;)
means = selector.cv_results_[&#39;mean_test_score&#39;]
stds = selector.cv_results_[&#39;std_test_score&#39;]
for mean, std, params in zip(means, stds, selector.cv_results_[&#39;params&#39;]):
    print(&#39;%0.3f (+/-%0.03f) for %r&#39; % (mean, std * 2, params))
    print()

classifier_3 = SVC(kernel=&#39;rbf&#39;, C=100)
clf_3 = classifier_3.fit(new_train_features, y_train)
y_pred_final = clf_3.predict(new_test_features)
model3_score = accuracy_score(y_test, y_pred_final)

print(&#39;Accuracy score:&#39;, model3_score)

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/human-activity/8.jpg&#34; alt=&#34;plot8&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As we can see this is the best hyperparameter {&amp;lsquo;C&amp;rsquo;: 100, &amp;lsquo;kernel&amp;rsquo;: &amp;lsquo;rbf&amp;rsquo;} with accuracy of 98.38%&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-and-data&#34;&gt;Code and Data&lt;/h2&gt;
&lt;p&gt;Full Code and Dataset can be found &lt;a class=&#34;link&#34; href=&#34;https://github.com/shubendu/human-activity-recognition&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/h2&gt;
&lt;p&gt;Congratulations if you have reached her :) We can try tree based models such as random forest and GBDT. If You have any question regarding this blog feel free to contact me on my website&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://greyatom.com/&#34;&gt;https://greyatom.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.appliedaicourse.com/&#34;&gt;https://www.appliedaicourse.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Financial Distress Prediction using SMOTE</title>
        <link>https://shubendu.github.io/p/financial-distress-prediction-using-smote/</link>
        <pubDate>Sat, 11 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://shubendu.github.io/p/financial-distress-prediction-using-smote/</guid>
        <description>&lt;h1 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h1&gt;
&lt;p&gt;Credit scoring algorithms, which makes a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. We will predicting the probability that somebody will experience financial distress in the next two years.&lt;/p&gt;
&lt;h1 id=&#34;dataset&#34;&gt;Dataset&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SeriousDlqin2yrs&lt;/td&gt;
&lt;td&gt;Person experienced 90 days past due delinquency or worse&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RevolvingUtilizationOfUnsecuredLines&lt;/td&gt;
&lt;td&gt;Total balance on credit cards and personal lines of credit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td&gt;Age of borrower in years&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberOfTime30-59DaysPastDueNotWorse&lt;/td&gt;
&lt;td&gt;Number of times borrower has been 30-59 days past due but no worse in the last 2 years&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DebtRatio&lt;/td&gt;
&lt;td&gt;Monthly debt payments, alimony,living costs divided by monthy gross income&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MonthlyIncome&lt;/td&gt;
&lt;td&gt;Monthly Income&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberOfOpenCreditLinesAndLoans&lt;/td&gt;
&lt;td&gt;Number of Open loans (installment like car loan or mortgage) and Lines of credit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberOfTimes90DaysLate&lt;/td&gt;
&lt;td&gt;Number of times borrower has been 90 days or more past due&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberRealEstateLoansOrLines&lt;/td&gt;
&lt;td&gt;Number of mortgage and real estate loans including home equity lines of credit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberOfTime60-89DaysPastDueNotWorse&lt;/td&gt;
&lt;td&gt;Number of times borrower has been 60-89 days past due but no worse in the last 2 years&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NumberOfDependents&lt;/td&gt;
&lt;td&gt;Number of dependents in family excluding themselves&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;importing-libraries&#34;&gt;Importing libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;1-eda&#34;&gt;1. EDA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;11-loading-dataset-and-splitting-into-train-test-sets&#34;&gt;1.1 Loading Dataset and splitting into train test sets&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First we&amp;rsquo;ll load the dataset with pandas and split it into X and y variable where X contains our features and y contains our target variable. This can be done by the following code&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df = pd.read_csv(&#39;financial.csv&#39;).drop(&#39;Unnamed: 0&#39;,1)
X = df.drop([&#39;SeriousDlqin2yrs&#39;],axis = 1)
y = df[&#39;SeriousDlqin2yrs&#39;]
count = y.value_counts()
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 6)
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;12-visualising-the-data&#34;&gt;1.2 Visualising the data&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Visualization is an important part as you can get an idea of the data just by looking at different graphs. We will try to get an idea of how our features are related to the dependent variable and get some insights about the data using scatter plot. The below code will do that.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cols = list(X.columns)
fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10,25))
for i in range(0,5):
    for j in range(0,2):
        col= cols[i * 2 + j]
        axes[i,j].set_title(col)
        axes[i,j].scatter(X_train[col],y_train)
        axes[i,j].set_xlabel(col)
        axes[i,j].set_ylabel(&#39;SeriousDlqin2yrs&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/1.jpg&#34; alt=&#34;scatter plot&#34;  /&gt;
&lt;img src=&#34;https://shubendu.github.io/img/financial/2.jpg&#34; alt=&#34;scatter plot2&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;13-value-count-of-classes&#34;&gt;1.3 Value count of classes&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we can see our dataset is highly imbalanced&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sns.countplot(df[&#39;SeriousDlqin2yrs&#39;])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/imbalance.jpg&#34; alt=&#34;value counts&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-data-preprocessing&#34;&gt;2. Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;21-checking-for-missing-values-and-replace-it-with-appropriate-values&#34;&gt;2.1 Checking for missing values and replace it with appropriate values&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Real-world data often has missing values. Data can have missing values for a number of reasons such as observations that were not recorded and data corruption. Handling missing data is important as many machine learning algorithms do not support data with missing values. We will check for missing values and replace them with appropriate values.The below code will do the task.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(X_train.isnull().sum())
X_train[&#39;MonthlyIncome&#39;].fillna(X_train[&#39;MonthlyIncome&#39;].median(),inplace = True)
X_train[&#39;NumberOfDependents&#39;].fillna(X_train[&#39;NumberOfDependents&#39;].median(),inplace = True)
X_test[&#39;MonthlyIncome&#39;].fillna(X_test[&#39;MonthlyIncome&#39;].median(),inplace = True)
X_test[&#39;NumberOfDependents&#39;].fillna(X_test[&#39;NumberOfDependents&#39;].median(),inplace = True)
print(X_test.isnull().sum())
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/fillna.jpg&#34; alt=&#34;fillna&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;22-feature-selection&#34;&gt;2.2 Feature selection&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We always try to reduce our features to the minimum number of most significant features. The basic rule of feature selection is that we need to select features which are highly correlated to the dependent variable and also not highly correlated with each other as they show the same trend. We do this with the help of the correlation matrix. So, we will find the features which are highly correlated and select the most significant features amongst them.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;corr = X_train.corr()
plt.figure(figsize=(14,12))
sns.heatmap(corr, annot=True, fmt=&amp;quot;.2g&amp;quot;)
X_train.drop([&#39;NumberOfTime30-59DaysPastDueNotWorse&#39;,&#39;NumberOfTime60-89DaysPastDueNotWorse&#39;],axis = 1,inplace=True)
X_test.drop([&#39;NumberOfTime30-59DaysPastDueNotWorse&#39;,&#39;NumberOfTime60-89DaysPastDueNotWorse&#39;],axis = 1,inplace=True)

print(X_train.columns)
print(X_test.columns)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/corr.jpg&#34; alt=&#34;heatmap&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As we can see from the heat map that the features NumberOfTime30-59DaysPastDueNotWorse, NumberOfTimes90DaysLate and NumberOfTime60-89DaysPastDueNotWorse are highly correlated. So we are dropping NumberOfTime30-59DaysPastDueNotWorse and NumberOfTime60-89DaysPastDueNotWorse from X_train and X_test as well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;23-scaling-the-features&#34;&gt;2.3 Scaling the features&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While working with the learning model, it is important to scale the features to a range which is centered around zero so that the variance of the features are in the same range. If the feature’s variance is orders of magnitude more than the variance of other features, that particular feature might dominate other features in the dataset and our model will not train well which gives us bad model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3-model&#34;&gt;3. Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;31-predict-the-values-after-building-a-machine-learning-model&#34;&gt;3.1 Predict the values after building a Machine learning model&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Logistic regression is another technique borrowed by machine learning from the field of statistics.It is the go-to method for binary classification problems (problems with two class values). In this post we will discover the logistic regression algorithm for machine learning.This is a classification problem to predict whether somebody will face financial distress in the next two years. So, here we will train our data on a Logistic regression algorithm and try to correctly predict the class.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;log_reg = LogisticRegression()
log_reg.fit(X_train,y_train)
y_pred = log_reg.predict(X_test)
accuracy = accuracy_score(y_test,y_pred)
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;32-is-our-prediction-right&#34;&gt;3.2 Is our prediction right?&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simply, building a predictive model is not your motive. But, creating and selecting a model which gives high accuracy. Hence, it is crucial to check accuracy of the model and we consider different kinds of metrics to evaluate our models. The choice of metric completely depends on the type of model and the implementation plan of the model. After you are finished building your model, these metrics will help you in evaluating your model accuracy.We will print the classification report of our model. This can be done by the following code&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;score = roc_auc_score(y_pred , y_test)
y_pred_proba = log_reg.predict_proba(X_test)[:,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label=&amp;quot;Logistic model, auc=&amp;quot;+str(auc))
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.legend(loc=4)
plt.show()
f1 = f1_score(y_test, log_reg.predict(X_test))
precision = precision_score(y_test, log_reg.predict(X_test))
recall = recall_score(y_test, log_reg.predict(X_test))
roc_auc = roc_auc_score(y_test, log_reg.predict(X_test))
print (&#39;Confusion_matrix&#39; + &#39;\n&#39;, confusion_matrix(y_test, log_reg.predict(X_test)))
print (&#39;Classification_report&#39; + &#39;\n&#39; + classification_report(y_test,y_pred))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/classification.jpg&#34; alt=&#34;classification&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As we can see our model is not working well in calss 1 category because this is an imbalanced data we will now try to fix this. Auc metric helps us to determine if our model is working well in imabalnced class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;4-smote&#34;&gt;4. SMOTE&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;41-balancing-the-dataset-using-smote&#34;&gt;4.1 Balancing the dataset using SMOTE&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we can see that the dataset is not balanced and it shows that 93% of customers will not face financial distress.In this situation, the predictive model developed using conventional machine learning algorithms could be biased and inaccurate.This happens because Machine Learning Algorithms are usually designed to improve accuracy by reducing the error. Thus, they do not take into account the class distribution / proportion or balance of classes. If we train our model on such an imbalanced data we will get incorrect predictions. To overcome this there are different methods such undersampling, oversampling and SMOTE. We will be using the SMOTE technique.Check for different evaluation metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;count = y.value_counts()
smote = SMOTE(random_state=9)
X_sample, y_sample = smote.fit_sample(X_train, y_train)
sns.countplot(y_sample)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/5.jpg&#34; alt=&#34;smote&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As we can see now our dataset is balanced now. Lets try applying new model on this dataset.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3 id=&#34;42-effect-of-applying-smote&#34;&gt;4.2 Effect of applying SMOTE?&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SMOTE is an over-sampling method. What it does is, it creates synthetic (not duplicate) samples of the minority class. Hence making the minority class equal to the majority class. After applying &amp;lsquo;SMOTE&amp;rsquo; we have balanced the data. We will use this balanced data for training our model and check whether the performance of our model has improved or not by comparing different evaluation parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;log_reg.fit(X_sample, y_sample)
y_pred = log_reg.predict(X_test)
score = roc_auc_score(y_pred , y_test)
y_pred_proba = log_reg.predict_proba(X_test)[:,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label=&amp;quot;Logistic model, auc=&amp;quot;+str(auc))
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.legend(loc=4)
plt.show()
f1 = f1_score(y_test, log_reg.predict(X_test))
precision = precision_score(y_test, log_reg.predict(X_test))
recall = recall_score(y_test, log_reg.predict(X_test))
# roc_auc = roc_auc_score(y_test, log_reg.predict(X_test))
print(&#39;Confusion matrix&#39; + &#39;\n&#39; ,confusion_matrix(y_test, log_reg.predict(X_test)))
print (&#39;Classification_report&#39; + &#39;\n&#39; + classification_report(y_test,y_pred))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/SMOTE_report.jpg&#34; alt=&#34;smote&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you compare previos classification report to this one you can observe that our model has imporved pretty much for our minority class. F1 score of class 1 has been improved from 3% to 21% , thats quite an improvemnt!! See the power of SMOTE.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;5-random-forest-algorithm&#34;&gt;5. Random Forest Algorithm&lt;/h2&gt;
&lt;p&gt;Random Forrest is a bagging technique which uses Decision Tree as the base model. The performance of our Logistic regression model has signifiacntly improved after balancing the data, lets check can we furthur improve it by using a Random Forrest model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rf = RandomForestClassifier(random_state=9)
rf.fit(X_sample, y_sample)
y_pred = rf.predict(X_test)
f1 = f1_score(y_test, rf.predict(X_test))
precison = precision_score(y_test, rf.predict(X_test))
recall = recall_score(y_test, rf.predict(X_test))
score = roc_auc_score(y_test, rf.predict(X_test))
print (&#39;Confusion_matrix&#39; + &#39;\n&#39;,confusion_matrix(y_test, rf.predict(X_test)))
print (&#39;Classification_report&#39; + &#39;\n&#39; + classification_report(y_test,y_pred))
score = roc_auc_score(y_pred , y_test)
y_pred_proba = rf.predict_proba(X_test)[:,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label=&amp;quot;Random_forest model, auc=&amp;quot;+str(auc))
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.legend(loc=4)
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://shubendu.github.io/img/financial/random-forest.jpg&#34; alt=&#34;random forest&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Observation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As you can see our model has been improved.F1 score of class 1 increased from 21% to 33%.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-and-data&#34;&gt;Code and Data&lt;/h2&gt;
&lt;p&gt;Full Code and Dataset can be found &lt;a class=&#34;link&#34; href=&#34;https://github.com/shubendu/Financial-distress-prediction-using-SMOTE&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/h2&gt;
&lt;p&gt;Congratulations if you have reached her :) You can try to increase the f1 score of minority class by hyperparameter tuning of random forest. Also you can try using xgboost. If You have any question regarding this blog feel free to contact me on my website&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://greyatom.com/&#34;&gt;https://greyatom.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.appliedaicourse.com/&#34;&gt;https://www.appliedaicourse.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
