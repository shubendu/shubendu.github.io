[{"content":"Problem Twitter data is massive and as such analyzing twitter data is a mammoth undertaking. the cleaning and pre-processing of Twitter. Political polarization and reactions to new products are probably some of the biggest use-cases of twitter data analytics. We have dataset containing user tweets. These tweets can be negative (0) and positive (4). Based on the obtained data our goal is to identify the sentiment/polarity of the tweets.\nAbout Dataset There are 6 columns/features in the dataset and are described below:\n target: the polarity of the tweet (0 = negative, 4 = positive) ids: The id of the tweet ( ex :2087) date: the date of the tweet (ex: Sat May 16 23:58:44 UTC 2009) flag: The query (lyx). If there is no query, then this value is NO_QUERY. user: the user-name of the user that tweeted text: the text of the tweet  Importing libraries import pandas as pd\rimport numpy as np\rimport os\rimport matplotlib.pyplot as plt\rimport seaborn as sns\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.metrics import precision_recall_fscore_support as error_metric\rfrom nltk.corpus import stopwords\rimport nltk\rfrom string import punctuation\rfrom nltk.stem.porter import *\rfrom wordcloud import WordCloud\rimport matplotlib.pyplot as plt\rfrom sklearn.feature_extraction.text import TfidfVectorizer\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.metrics import accuracy_score, classification_report\rfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\rimport warnings\rwarnings.filterwarnings('ignore')\rEDA   Loading Dataset ,checking for null values and sampling from data   The first step - you know the drill by now - load the dataset and see how it looks like. In this task, we are basically looking for glimpse of overall data and look at the null values if they are present. Since the data 1600000 rows, We will be taking sample of 5000 tweets for the sake of easy computation power.\ncolumns = ['target','ids','date','flag','user','text']\rdata = pd.read_csv('file.csv', encoding = 'latin-1', header = None)\rdata.columns = columns\rdata.shape\rdata = data.sample(n=5000, random_state=2)\rdata\rSentiment Analysis simplified In layman\u0026rsquo;s terms sentiment analysis is basically figuring out the sentiment of a particular text. In the case with our problem statement, it is about finding out if a particular tweet is positive or negative, happy or unhappy (based on a score of course).\nApproach towards dealing with sentiment analysis\nThe key to sentiment analysis is finding a number which indicates a level of positivity or negativity usually a number between 0 to 1, as it is obvious that computers cannot understand emotions such as happy or sad. However, a number could substitute an emotion and the value of this number signifies the magnitude of emotion in a direction; ex: very happy is above 0.9, happy is between 0.75 and 0.9, neutral is 0.5 and so on. The main challenge is how to obtain this number correctly with the help of given data.\nIn supervised setting you already have information as labels are already present in the training phase. First you would clean the data of unwanted substances like stopwords, punctuation marks etc. and then extract features by converting words into numbers. You can do this extraction step in various different ways like Bag-of-Words, TF-IDF with or without n-grams approach. Then you feed some sort of classifier which outputs a labels based on some internal scoring mechanism.\nWhy use sentiment analysis? It’s estimated that 80% of the world’s data is unstructured and not organized in a pre-defined manner. Most of this comes from text data, like emails, support tickets, chats, social media, surveys, articles, and documents. These texts are usually difficult, time-consuming and expensive to analyze, understand, and sort through.\nSentiment analysis systems allows companies to make sense of this sea of unstructured text by automating business processes, getting actionable insights, and saving hours of manual data processing, in other words, by making teams more efficient.\nWith the help of sentiment analysis, unstructured information could be automatically transformed into structured data of public opinions about products, services, brands, politics, or any topic that people can express opinions about. This data can be very useful for commercial applications like marketing analysis, public relations, product reviews, net promoter scoring, product feedback, and customer service.\nIssues with sentiment analysis Computers have a hard time carrying out sentiment analysis tasks. Some pressing issues with performing sentiment analysis are:\n Sarcasm: It is one of the most difficult sentiments to interpret properly. Example: \u0026ldquo;It was awesome for the week that it worked.\u0026rdquo; Relative sentiment: It is not a classic negative, but can be a negative nonetheless. Example: \u0026ldquo;I bought an iPhone\u0026rdquo; is good for Apple, but not for other mobile companies. Compound or multidimensional sentiment: These types contain positives and negatives in the same phrase. Example: \u0026ldquo;I love Mad Men, but hate the misleading episode trailers.\u0026rdquo; The above sentence consists of two polarities, i.e., Positive as well as Negative. So how do we conclude whether the review was Positive or Negative? Use of emoticons: Heavy use of emoticons (which have sentiment values) in social media texts like that of Twitter and Facebook also makes text analysis difficult.  Data Preprocessing   Tackling user handles   Tweets can be directed to any other person with username/user handle \u0026ldquo;NAME\u0026rdquo; @NAME. Consider the tweet \u0026lsquo;@brodiejay OH IM GOING THERE! Wow Mona Vale is a real place afterall! I know it sucks Mville only does the slow train pffft \u0026lsquo;. Here, \u0026lsquo;@brodiejay\u0026rsquo; is the user handle or username of the person to whom that particular tweet was directed/referred. If you take another tweet \u0026ldquo;my baby\u0026rsquo;s growing up \u0026ldquo;, it doesn\u0026rsquo;t contain any such user handle. So, in the dataset, user handles are present, but not in all observations.\nFrom basic intuition it is clear that user handles have little to zero contribution towards sentiment formation. So, it is a good idea to remove them altogether from the data. Getting rid of user handles also helps in reduction of the term-frequency matrix that gets generated. This will directly boost up calculation speed and also performance as unnecessary features will not be generated. Below code will clean our tweets.\ndef remove_pattern(input_txt, pattern):\rr = re.findall(pattern, input_txt)\rfor i in r:\rinput_txt = re.sub(i, '', input_txt)\rreturn input_txt data['clean_text'] = data['text'].apply(lambda row:remove_pattern(row, \u0026quot;@[\\w]*\u0026quot;))\rdata.head(5)\r  Text Preprocessing   This data can now be further preprocessed of unwanted entries which will make the data more suitable for carrying out analysis. Below discussed in brief are the preprocessing techniques that you will be carrying out at the end of this topic.\n  Tokenization- Tokenization describes splitting paragraphs into sentences, or sentences into individual pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens. Most of what we are going to do with language relies on ﬁrst separating out such tokens separately so that subsequent preprocessing steps can be successfully done.\n  Stopwords removal- Stopwords are the most common words in a language like \u0026lsquo;the\u0026rsquo;, \u0026lsquo;a\u0026rsquo;, \u0026lsquo;on\u0026rsquo;, \u0026lsquo;is\u0026rsquo;, \u0026lsquo;all\u0026rsquo;. These words do not carry important meaning and so are usually removed from texts.\n  Stemming- Much of natural language machine learning is about sentiment of the text. Stemming is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look). The main two algorithms are Porter stemming algorithm (removes common morphological and inflexional endings from words and Lancaster stemming algorithm (a more aggressive stemming algorithm). Below Code will tokenize our clean text then removing stopwords from nltk, after that we will aply stemming.\n  stop_words = list(set(stopwords.words('english')))+list(punctuation)+['``', \u0026quot;'s\u0026quot;, \u0026quot;...\u0026quot;, \u0026quot;n't\u0026quot;]\rdata['tokenized_text'] = [nltk.word_tokenize(x) for x in data['clean_text']]\rdata['tokenized_text'] = data['tokenized_text'].apply(lambda row: [word for word in row if word not in stop_words])\rstemmer = PorterStemmer()\rdata['tokenized_text'] = data['tokenized_text'].apply(lambda x: [stemmer.stem(i) for i in x])\rdata['tokenized_text'] = data['tokenized_text'].apply(lambda x: ' '.join(x))\rdata.head()\rObservation:\n As you can see tokenized_text\u0026rsquo;s 1st row is converted into \u0026ldquo;OH IM go there\u0026rdquo; from \u0026ldquo;OH IM GOING THERE!\u0026rdquo;\n Word Cloud   Word cloud of all data   It is a visualisation method that displays how frequently words appear in a given body of text, by making the size of each word proportional to its frequency. All the words are then arranged in a cluster or cloud of words. Alternatively, the words can also be arranged in any format: horizontal lines, columns or within a shape.\nNow that you have an idea about how to use a wordcloud, lets do a simple task to generate a wordcloud to observe the most frequently occuring words.\nall_words = ' '.join([text for text in data['tokenized_text']])\r# generate wordcloud object\rwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\rplt.figure(figsize=(20, 12))\rplt.imshow(wordcloud, interpolation=\u0026quot;bilinear\u0026quot;)\rplt.axis('off')\rplt.show()\r  Word cloud for negative data   # negative tweets\rneg_words = ' '.join([text for text in data['tokenized_text'][data['target'] == 0]])\r# generate wordcloud object for negative tweets\rneg_wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(neg_words)\rplt.figure(figsize=(20, 12))\rplt.imshow(neg_wordcloud, interpolation=\u0026quot;bilinear\u0026quot;)\rplt.axis('off')\rplt.show()\r  Word cloud for postive data   # positive tweets\rpos_words = ' '.join([text for text in data['tokenized_text'][data['target'] == 4]])\r# generate wordcloud object for negative tweets\rpos_wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(pos_words)\rplt.figure(figsize=(20, 12))\rplt.imshow(pos_wordcloud, interpolation=\u0026quot;bilinear\u0026quot;)\rplt.axis('off')\rplt.show()\rObservation:\n As you can see in negative word cloud there are lots of negative words such as fuck, bad, tire etc and in postive word cloud there are many good words.\n Model Building Logistic Regression for predicting sentiments These steps enabled us with data in proper format as well as some interesting insights about certain keywords. This cleaned form of data can now be fed to a machine learning algorithm to classify tweets into categories. Since we are already familiar with the workflow of classifying texts, we will be only outlining the steps associated with it. The steps are as follows:\n Splitting into training and test sets Construct a term-document matrix (can be done by Bag of Words or TF-IDF) Fitting a classifier on training data Predicting on test data Evaluating classifier performance  # ratio to split into training and test set\rratio = int(len(data)*0.75)\r# logistic regression model\rlogreg = LogisticRegression(random_state=2)\r# TF-IDF feature matrix\rtfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\r# fit and transform tweets\rtweets = tfidf_vectorizer.fit_transform(data['tokenized_text'])\r# convert positive tweets to 1's\rdata['target'] = data['target'].apply(lambda x: 1 if x==4 else x)\r# split into train and test\rX_train = tweets[:ratio,:]\rX_test = tweets[ratio:,:]\ry_train = data['target'].iloc[:ratio]\ry_test = data['target'].iloc[ratio:]\r# fit on training data\rlogreg.fit(X_train,y_train)\r# make predictions\rprediction = logreg.predict_proba(X_test)\rprediction_int = prediction[:,1] \u0026gt;= 0.3\rprediction_int = prediction_int.astype(np.int)\rreport = classification_report(y_test,prediction_int)\r# print out classification_report\rprint(report)\rTextBlob There are numerous libraries out there for dealing with text data. One of them is TextBlob which is built on the shoulders of NLTK and Pattern. A big advantage of TextBlob is it is easy to learn and offers a lot of features like sentiment analysis, pos-tagging, noun phrase extraction, etc. It has now become a go-to library for many users who are performing NLP tasks.\nFeatures of TextBlob\nThe documentation page of TextBlob says; TextBlob aims to provide access to common text-processing operations through a familiar interface. You can treat TextBlob objects as if they were Python strings that learned how to do Natural Language Processing.\n  Sentiment Analysis with TextBlob   You can carry out sentiment analysis with textBlob too. The TextBlob object has an attribute sentiment that returns a tuple of the form Sentiment (polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0] with negative values corresponding to negative sentiments and positive values to positive sentiments. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\nBut how it does this prediction of sentiment scoring? Well, it has a training set with preclassified movie reviews, so when you give a new text for analysis, it uses NaiveBayes classifier to classify the new text\u0026rsquo;s polarity in pos and neg probabilities.\n# list to store polarities\rtb_polarity = []\r# loop over tweets\rfor sentence in data['tokenized_text']:\rtemp = TextBlob(sentence)\rtb_polarity.append(temp.sentiment[0])\r# new column to store polarity data['tb_polarity'] = tb_polarity\rdata.head(10)\rObservation\n You can compare the target values with tb_polarity and see how much TextBlob is accurate\n VaderSentiment Another library for out of the box sentiment analysis is vaderSentiment. It is an open sourced python library where VADER stands for Valence Aware Dictionary and sEntiment Reasoner. With VADER you can be up and running performing sentiment classification very quickly even if you don\u0026rsquo;t have positive and negative text examples to train a classifier or want to write custom code to search for words in a sentiment lexicon. VADER is also computationally efficient when compared to other Machine Learning and Deep Learning approaches.\nVADER performs well on text originating in social media and is described fully in a paper entitled VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. published at ICWSM-14. VADER is able to include sentiment from emoticons (e.g, :-)), sentiment-related acronyms (e.g, LOL) and slang (e.g, meh). The developers of VADER have used Amazon’s Mechanical Turk to get most of their ratings, You can find complete details on their Github Page.\nOutput of VADER VADER produces four sentiment metrics from these word ratings. The first three metrics; positive, neutral and negative, represent the proportion of the text that falls into those categories. The final metric, the compound score, is the sum of all of the lexicon ratings which have been standardised to range between -1 and 1.\nHow does VADER work? First step is to import SentimentIntensityAnalyzer from vaderSentiment.vaderSentiment. Then initialize an object of SentimentIntensityAnalyzer() and use its .polarity_scores() on a given text to find about its four metrics. Below given is a code snippet:\nanalyser = SentimentIntensityAnalyzer()\r# empty list to store VADER polarities\rvs_polarity = []\r# loop over tweets\rfor sentence in data['tokenized_text']:\rvs_polarity.append(analyser.polarity_scores(sentence)['compound'])\r# add new column `'vs_polarity'` to data\rdata['vs_polarity'] = vs_polarity\rdata.head(10)\rObservation:\n you can also compare the vs_polarity to the target variables.\n  Code and Data Full Code and Dataset can be found here\nConclusion and Future Work Congratulations if you have reached her :) We can try tree based models such as random forest and GBDT and other types of feature transformation such as word2vec. If You have any question regarding this blog feel free to contact me on my website\nReference  https://greyatom.com/  ","date":"2020-02-13T00:00:00Z","image":"https://shubendu.github.io/p/sentiment-analysis-of-tweets/tweets_hu4dde5be826bc3f051bb55b4c098b9fae_66995_120x120_fill_q75_box_smart1.jpg","permalink":"https://shubendu.github.io/p/sentiment-analysis-of-tweets/","title":"Sentiment analysis of tweets"},{"content":"Problem For this project we will be exploring the publicly available data from LendingClub.com. Lending Club connects people who need money (borrowers) with people who have money (investors). As an investor one would want to invest in people who showed a profile of having a high probability of paying the amount back. Using Decision Tree model, classify whether or not the borrower paid back their loan in full.\nAbout Dataset The snapshot of the data we will be working on:\n 1 \n   Feature Description     customer.id ID of the customer   credit.policy If the customer meets the credit underwriting criteria of LendingClub.com or not   purpose The purpose of the loan(takes values :\u0026ldquo;creditcard\u0026rdquo;, \u0026ldquo;debtconsolidation\u0026rdquo;, \u0026ldquo;educational\u0026rdquo;, \u0026ldquo;majorpurchase\u0026rdquo;, \u0026ldquo;smallbusiness\u0026rdquo;, and \u0026ldquo;all_other\u0026rdquo;).   int.rate The interest rate of the loan   installment The monthly installments owed by the borrower if the loan is funded   log.annual.inc The natural log of the self-reported annual income of the borrower   dti The debt-to-income ratio of the borrower (amount of debt divided by annual income)   fico The FICO credit score of the borrower   days.with.cr.line The number of days the borrower has had a credit line.   revol.bal The borrower\u0026rsquo;s revolving balance (amount unpaid at the end of the credit card billing cycle)   revol.util The borrower\u0026rsquo;s revolving line utilization rate (the amount of the credit line used relative to total credit available)   pub.rec The borrower\u0026rsquo;s number of derogatory public records (bankruptcy filings, tax liens, or judgments)   inq.last.6mths The borrower\u0026rsquo;s number of inquiries by creditors in the last 6 months   delinq.2yrs The number of times the borrower had been 30+ days past due on a payment in the past 2 years   paid.back.loan Whether the user has paid back loan    Importing libraries import pandas as pd\rfrom sklearn.model_selection import train_test_split\rimport warnings\rwarnings.filterwarnings('ignore')\rimport matplotlib.pyplot as plt\rimport numpy as np\rfrom sklearn.preprocessing import LabelEncoder\rimport seaborn as sns\rfrom sklearn.tree import DecisionTreeClassifier\rfrom sklearn.model_selection import GridSearchCV\rLoading Dataset ,checking for null values The first step - you know the drill by now - load the dataset and see how it looks like. In this task, we are basically looking for glimpse of overall data and look at the null values if they are present also some statistical representation of our data.\ndata = pd.read_csv('loan.csv')\rdata.head().T\rdata.describe()\rdata.info()\r 2 \n 3 \n 4 \nObservation\n We have no null values. We\u0026rsquo;ll drop the customer id, as it is of no use for our model and we have both numeric and categorical types of data which we will further preprocess.\n Splitting the data Let\u0026rsquo;s split the data into train and test\nX = data.drop(['customer.id','paid.back.loan'], axis = 1)\ry = data['paid.back.loan']\rX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state = 0)\rprint(X_train.shape , y_train.shape)\rprint(X_test.shape, y_test.shape)\r 5 \nTarget variable distribution The distribution of \u0026ldquo;paid.back.loan\u0026rdquo; and plotting barplot.\n 6 \nObservation:\n We can see that 5639 people have paid back loan while 1065 people not paid back the loan.\n Feature Enginnering We need to preprocess data beofre feature engineering as we can see that \u0026ldquo;int.rate\u0026rdquo; column has percentage symbol which need to be remove and later I am dividing that column with 100 to get the actual percentage values. After that I will be seperating the data into numeric and categorical dataframe.\n#Removing the last character from the values in column\rX_train['int.rate'] = X_train['int.rate'].map(lambda x: str(x)[:-1])\r#Dividing the column values by 100\rX_train['int.rate']=X_train['int.rate'].astype(float)/100\r#Removing the last character from the values in column\rX_test['int.rate'] = X_test['int.rate'].map(lambda x: str(x)[:-1])\r#Dividing the column values by 100\rX_test['int.rate']=X_test['int.rate'].astype(float)/100\r#Storing all the numerical type columns in 'num_df'\rnum_df=X_train.select_dtypes(include=['number']).copy()\r#Storing all the categorical type columns in 'cat_df'\rcat_df=X_train.select_dtypes(include=['object']).copy()\rFeature Visualisation Now we can visualise the distribuiton of our numeric dataset in different calss variable. Below code will do the job\ncols=list(num_df.columns)\rfor i in range(9): #Plotting boxplot\rsns.boxplot(x=y_train,y=num_df[cols[i]],ax=axes[i])\r#Avoiding subplots overlapping\rfig.tight_layout()  7 \n 8 \nObservation:\n Most of our features has different distribution for our class variable which is good for our model\n  Lets\u0026rsquo;s visualise the categorical features as well. I will be plotting using seaborn to see how our distribution differs in different class.\ncols=list(cat_df.columns)\r#Looping through rows\rfor i in range(0,2):\r#Looping through columns\rfor j in range(0,2):\r#Plotting count plot\rsns.countplot(x=X_train[cols[i*2+j]], hue=y_train,ax=axes[i,j]) #Avoiding subplots overlapping\rfig.tight_layout()  9   10 \nObservation:\n We can see that the major reason that stands common for the majority of customers who have applied for a loan is debt_consolidation which means taking one loan to payoff there other loans.\n 1\nModel Building Let\u0026rsquo;s Apply the Decision Tree classifier to our dataset. We will encode the categorical features using label encoder.\nfor col in cat_df.columns:\r#Filling null values with 'NA'\rX_train[col].fillna('NA',inplace=True)\r#Initalising a label encoder object\rle=LabelEncoder()\r#Fitting and transforming the column in X_train with 'le'\rX_train[col]=le.fit_transform(X_train[col]) #Filling null values with 'NA'\rX_test[col].fillna('NA',inplace=True)\r#Fitting the column in X_test with 'le'\rX_test[col]=le.transform(X_test[col]) # Replacing the values of y_train\ry_train.replace({'No':0,'Yes':1},inplace=True)\r# Replacing the values of y_test\ry_test.replace({'No':0,'Yes':1},inplace=True)\r#Initialising 'Decision Tree' model model=DecisionTreeClassifier(random_state=0)\r#Training the 'Decision Tree' model\rmodel.fit(X_train, y_train)\r#Finding the accuracy of 'Decision Tree' model\racc=model.score(X_test, y_test)\r#Printing the accuracy\rprint(acc)\r 11 \nObservation:\n We have total 74% accuracy on our model without having any hyperparameter tuning.\n Decision Tree Pruning Let\u0026rsquo;s see if pruning of decision tree improves its accuracy. We will use grid search to do the optimum pruning.\nparameter_grid = {'max_depth': np.arange(3,10), 'min_samples_leaf': range(10,50,10)}\r#Code starts here\r#Initialising 'Decision Tree' model\rmodel_2 = DecisionTreeClassifier(random_state=0)\r#Applying Grid Search of hyper-parameters and finding the optimum 'Decision Tree' model\rp_tree = GridSearchCV(model_2, parameter_grid, cv=5)\r#Training the optimum 'Decision Tree' model\rp_tree.fit(X_train, y_train)\r#Finding the accuracy of the optimum 'Decision Tree' model\racc_2 = p_tree.score(X_test, y_test)\r#Printing the accuracy\rprint(acc_2)\r 12 \nObservation:\n Great our accuracy has improved drastically.\n Tree visualising we can also visualise our tree.\n#Importing header files\rfrom io import StringIO\rfrom sklearn.tree import export_graphviz\rfrom sklearn import tree\rfrom sklearn import metrics\rfrom IPython.display import Image\rimport pydotplus\r#Creating DOT data\rdot_data = export_graphviz(decision_tree=p_tree.best_estimator_, out_file=None, feature_names=X.columns, filled = True, class_names=['loan_paid_back_yes','loan_paid_back_no'])\r#Drawing graph\rgraph_big = pydotplus.graph_from_dot_data(dot_data) #Displaying graph\r# show graph - do not delete/modify the code below this line\rimg_path = user_data_dir+'/file.png'\rgraph_big.write_png(img_path)\rplt.figure(figsize=(20,15))\rplt.imshow(plt.imread(img_path))\rplt.axis('off')\rplt.show()  13 \n Code and Data Full Code and Dataset can be found here\nConclusion and Future Work Congratulations if you have reached her :) We can try GBDT and XgBoost to increase our model accuracy.\nReference  https://greyatom.com/  ","date":"2020-01-24T00:00:00Z","image":"https://shubendu.github.io/p/test-yolo/loan_hu1e6b80bc4728ae0d0aaf8a2d7dcdfe77_79453_120x120_fill_q75_box_smart1.jpg","permalink":"https://shubendu.github.io/p/test-yolo/","title":"Loan Defaulters"},{"content":"Problem Statement The Human Activity Recognition database was built from the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. The objective is to classify activities into one of the six activities performed.\nAbout Dataset The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKINGUPSTAIRS, WALKINGDOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.\nThe sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.\nAttribute information For each record in the dataset the following is provided:\n  Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\n  Triaxial Angular velocity from the gyroscope.\n  A 561-feature vector with time and frequency domain variables.\n  Its activity label.\n  An identifier of the subject who carried out the experiment.\n  Importing libraries import pandas as pd\rimport numpy as np\rimport os\rimport matplotlib.pyplot as plt\rimport seaborn as sns\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.preprocessing import LabelEncoder\rfrom sklearn.svm import SVC\rfrom sklearn.metrics import precision_recall_fscore_support as error_metric\rfrom sklearn.metrics import confusion_matrix, accuracy_score\rfrom sklearn.feature_selection import SelectFromModel\rfrom sklearn.model_selection import GridSearchCV\rfrom sklearn.svm import LinearSVC\rimport warnings\rwarnings.filterwarnings('ignore')\r1. EDA 1.1 Loading Dataset ,checking for null values and statistical description The first step - you know the drill by now - load the dataset and see how it looks like. In this task, we are basically looking for descriptive statistics of overall data and look at the null values if they are present. Since the data is too large I will be only printiing till 25 columns.\ndata = pd.read_csv('file.csv')\rprint('Null Values In DataFrame: {}\\n'.format(data.isna().sum().sum()))\rdata.describe().T[:25]\rWe have 563 features and 10299 rows\nObservation:\n After observing the dataset statistical description majority of the values are having a minimum value of -1 and maximum value of 1 which brings us to the conclusion that scaling is not required in this case. Also the data set is free of any kind of null values.\n   1.2 Distribution of our class   Let\u0026rsquo;s get a little bit deeper dive into exploring the target feature via graphical methods.\nlabel = data['Activity']\rsns.countplot(x= label)\rplt.xticks(rotation=75);\rObservation:\n The class distribution are somewhat balanced which is a better thing on prediction and model building part.\n   1.3 How Long Does The Participant Use The Staircase?   Since the dataset has been created in an scientific environment nearly equal preconditions for the participants can be assumed. It is highly likely for the participants to have been walking up and down the same number of staircases. Bascially we are looking to calculate how much time the each participant has been spending while walking downstairs and upstairs as it seems to be most frequent activity. Let us investigate their activity durations. The description of the data states; \u0026ldquo;fixed-width sliding windows of 2.56 sec and 50% overlap\u0026rdquo; for each datapoint.Therefore a single datapoint is gathered every 1.28 sec.\ndata_copy = data.copy() data_copy['duration'] = ''\rduration_df = (data_copy.groupby([label[label.isin(['WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS'])], 'subject'])['duration'].count() * 1.28)\rduration_df = pd.DataFrame(duration_df)\rplot_data = duration_df.reset_index().sort_values('duration', ascending=False)\rplot_data['Activity'] = plot_data['Activity'].map({'WALKING_UPSTAIRS':'Upstairs', 'WALKING_DOWNSTAIRS':'Downstairs'})\rplt.figure(figsize=(15,5))\rsns.barplot(data=plot_data, x='subject', y='duration', hue='Activity')\rplt.title('Participants Compared By Their Staircase Walking Duration')\rplt.xlabel('Participants')\rplt.ylabel('Total Duration [s]')\rplt.show()\rObservation:\n Nearly all participants have more data for walking upstairs than downstairs. Assuming an equal number of up- and down-walks the participants need longer walking upstairs. Furthermore the range of the duration is narrow and adjusted to the conditions. A young person being ~50% fast in walking upstairs than an older one is reasonable.\n   1.4 Finding the correlation   We always try to reduce our features to the minimum number of most significant features. The basic rule of feature selection is that we need to select features which are highly correlated to the dependent variable and also not highly correlated with each other as they show the same trend. Correlation refers to the mutual relationship and association between quantities and it is generaly used to express one quantity in terms of its relationship with other quantities. This can either be Positive(variables change in the same direction), negative(variables change in opposite direction or neutral(No correlation). Let\u0026rsquo;s investigate the correlated pairs. We are storing the correlated pairs exculding self correlated pairs in the variable named as top_corr_feilds with a threshold of 0.8 for absolute correlation.\n\rfeature_cols = data.columns[: -2] correlated_values = data[feature_cols].corr()\rcorrelated_values = (correlated_values.stack().to_frame().reset_index().rename(columns={'level_0': 'Feature_1', 'level_1': 'Feature_2', 0:'Correlation_score'}))\rcorrelated_values['abs_correlation'] = correlated_values.Correlation_score.abs()\rtop_corr_fields = correlated_values.sort_values('Correlation_score', ascending = False).query('abs_correlation\u0026gt;0.8 ')\rtop_corr_fields = top_corr_fields[top_corr_fields['Feature_1'] != top_corr_fields['Feature_2']].reset_index(drop=True)\r2. Data Processing and baseline model   2.1 Data preparation and implementing baseline model   Now, we will be building a baseline model of svm which would take data with insignificant amount of data preprocessing which is to observe how our model is performing in non ideal situations.\nle = LabelEncoder()\rdata['Activity'] = le.fit_transform(data.Activity)\rX = data.iloc[:,:-1] y = data.iloc[:,-1]\rX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)\rclassifier = SVC()\rclf = classifier.fit(X_train, y_train)\ry_pred = clf.predict(X_test)\rprecision, recall, f_score, _ = error_metric(y_test, y_pred, average = 'weighted')\rmodel1_score = accuracy_score(y_test, y_pred)\rprint(model1_score)\rprint(precision, recall, f_score)\r  2.2 Feature selection and removing top correlated variables   We have near about 500+ features in our dataset. Not every feature contributes towards the model prediction hence we can cut down the no. of features according to their importance and then start a model building on top of it. This vague idea or process is called feature selection. We will be training a new scv model on new reduced dataset. We are reducing the features with the help of sklearn SelectFromModel. Lets see the accuracy metrics of this new trained model and shape of our new dataset.\nlsvc = LinearSVC(C = 0.01, penalty=\u0026quot;l1\u0026quot;, dual=False, random_state=42).fit(X_train, y_train)\rmodel_2 = SelectFromModel(lsvc, prefit=True)\rnew_train_features = model_2.transform(X_train)\rnew_test_features = model_2.transform(X_test)\rprint(new_train_features.shape,new_test_features.shape )\rclassifier_2 = SVC()\rclf_2 = classifier_2.fit(new_train_features, y_train)\ry_pred_new = clf_2.predict(new_test_features)\rmodel2_score =accuracy_score(y_test, y_pred_new)\rprecision, recall, f_score, _ = error_metric(y_test, y_pred_new, average='weighted')\rprint(model2_score)\rprint(precision, recall, f_score)\rObservation\n As we can see that our model is still working really well even after reduction of so many features. We have reduced features from 562 to 110 and you can compare the accuracy from previous model it is almost equivalent.\n 4. Hyperparameter Tuning for SVC with Grid Search and final model building. Model hyperparmeters are configurations that is external to the model and whose value cannot be estimated from data. We will use gridsearch here with both rbf and linear kernel and value of C from 0.1 to 100.\nparameters = {\r'kernel': ['linear', 'rbf'],\r'C': [100, 20, 1, 0.1]\r}\rselector = GridSearchCV(SVC(), parameters, scoring='accuracy') selector.fit(new_train_features, y_train)\rprint('Best parameter set found:')\rprint(selector.best_params_)\rprint('Detailed grid scores:')\rmeans = selector.cv_results_['mean_test_score']\rstds = selector.cv_results_['std_test_score']\rfor mean, std, params in zip(means, stds, selector.cv_results_['params']):\rprint('%0.3f (+/-%0.03f) for %r' % (mean, std * 2, params))\rprint()\rclassifier_3 = SVC(kernel='rbf', C=100)\rclf_3 = classifier_3.fit(new_train_features, y_train)\ry_pred_final = clf_3.predict(new_test_features)\rmodel3_score = accuracy_score(y_test, y_pred_final)\rprint('Accuracy score:', model3_score)\rObservation:\n As we can see this is the best hyperparameter {\u0026lsquo;C\u0026rsquo;: 100, \u0026lsquo;kernel\u0026rsquo;: \u0026lsquo;rbf\u0026rsquo;} with accuracy of 98.38%\n  Code and Data Full Code and Dataset can be found here\nConclusion and Future Work Congratulations if you have reached her :) We can try tree based models such as random forest and GBDT. If You have any question regarding this blog feel free to contact me on my website\nReference  https://greyatom.com/ https://www.appliedaicourse.com/  ","date":"2020-01-15T00:00:00Z","image":"https://shubendu.github.io/p/human-activity-recognition-with-smartphones/actt_hue74ba066313cb86b383ab414604436a4_113353_120x120_fill_q75_box_smart1.jpg","permalink":"https://shubendu.github.io/p/human-activity-recognition-with-smartphones/","title":"Human Activity Recognition with Smartphones"},{"content":"Problem Statement Credit scoring algorithms, which makes a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. We will predicting the probability that somebody will experience financial distress in the next two years.\nDataset    Feature Description     SeriousDlqin2yrs Person experienced 90 days past due delinquency or worse   RevolvingUtilizationOfUnsecuredLines Total balance on credit cards and personal lines of credit   age Age of borrower in years   NumberOfTime30-59DaysPastDueNotWorse Number of times borrower has been 30-59 days past due but no worse in the last 2 years   DebtRatio Monthly debt payments, alimony,living costs divided by monthy gross income   MonthlyIncome Monthly Income   NumberOfOpenCreditLinesAndLoans Number of Open loans (installment like car loan or mortgage) and Lines of credit   NumberOfTimes90DaysLate Number of times borrower has been 90 days or more past due   NumberRealEstateLoansOrLines Number of mortgage and real estate loans including home equity lines of credit   NumberOfTime60-89DaysPastDueNotWorse Number of times borrower has been 60-89 days past due but no worse in the last 2 years   NumberOfDependents Number of dependents in family excluding themselves    Importing libraries import pandas as pd\rimport numpy as np\rimport os\rimport matplotlib.pyplot as plt\rimport seaborn as sns\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.metrics import accuracy_score\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.metrics import roc_auc_score\rfrom sklearn import metrics\rimport matplotlib.pyplot as plt\rfrom sklearn.metrics import f1_score, confusion_matrix, classification_report\rfrom sklearn.metrics import precision_score, recall_score\rfrom imblearn.over_sampling import SMOTE\rfrom sklearn.ensemble import RandomForestClassifier\rimport warnings\rwarnings.filterwarnings('ignore')\r1. EDA   1.1 Loading Dataset and splitting into train test sets   First we\u0026rsquo;ll load the dataset with pandas and split it into X and y variable where X contains our features and y contains our target variable. This can be done by the following code\ndf = pd.read_csv('financial.csv').drop('Unnamed: 0',1)\rX = df.drop(['SeriousDlqin2yrs'],axis = 1)\ry = df['SeriousDlqin2yrs']\rcount = y.value_counts()\rX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 6)\r  1.2 Visualising the data   Visualization is an important part as you can get an idea of the data just by looking at different graphs. We will try to get an idea of how our features are related to the dependent variable and get some insights about the data using scatter plot. The below code will do that.\ncols = list(X.columns)\rfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10,25))\rfor i in range(0,5):\rfor j in range(0,2):\rcol= cols[i * 2 + j]\raxes[i,j].set_title(col)\raxes[i,j].scatter(X_train[col],y_train)\raxes[i,j].set_xlabel(col)\raxes[i,j].set_ylabel('SeriousDlqin2yrs')\r  1.3 Value count of classes   As we can see our dataset is highly imbalanced\nsns.countplot(df['SeriousDlqin2yrs'])\r2. Data Preprocessing   2.1 Checking for missing values and replace it with appropriate values   Real-world data often has missing values. Data can have missing values for a number of reasons such as observations that were not recorded and data corruption. Handling missing data is important as many machine learning algorithms do not support data with missing values. We will check for missing values and replace them with appropriate values.The below code will do the task.\nprint(X_train.isnull().sum())\rX_train['MonthlyIncome'].fillna(X_train['MonthlyIncome'].median(),inplace = True)\rX_train['NumberOfDependents'].fillna(X_train['NumberOfDependents'].median(),inplace = True)\rX_test['MonthlyIncome'].fillna(X_test['MonthlyIncome'].median(),inplace = True)\rX_test['NumberOfDependents'].fillna(X_test['NumberOfDependents'].median(),inplace = True)\rprint(X_test.isnull().sum())\r  2.2 Feature selection   We always try to reduce our features to the minimum number of most significant features. The basic rule of feature selection is that we need to select features which are highly correlated to the dependent variable and also not highly correlated with each other as they show the same trend. We do this with the help of the correlation matrix. So, we will find the features which are highly correlated and select the most significant features amongst them.\ncorr = X_train.corr()\rplt.figure(figsize=(14,12))\rsns.heatmap(corr, annot=True, fmt=\u0026quot;.2g\u0026quot;)\rX_train.drop(['NumberOfTime30-59DaysPastDueNotWorse','NumberOfTime60-89DaysPastDueNotWorse'],axis = 1,inplace=True)\rX_test.drop(['NumberOfTime30-59DaysPastDueNotWorse','NumberOfTime60-89DaysPastDueNotWorse'],axis = 1,inplace=True)\rprint(X_train.columns)\rprint(X_test.columns)\rObservation:\n As we can see from the heat map that the features NumberOfTime30-59DaysPastDueNotWorse, NumberOfTimes90DaysLate and NumberOfTime60-89DaysPastDueNotWorse are highly correlated. So we are dropping NumberOfTime30-59DaysPastDueNotWorse and NumberOfTime60-89DaysPastDueNotWorse from X_train and X_test as well.\n   2.3 Scaling the features   While working with the learning model, it is important to scale the features to a range which is centered around zero so that the variance of the features are in the same range. If the feature’s variance is orders of magnitude more than the variance of other features, that particular feature might dominate other features in the dataset and our model will not train well which gives us bad model.\nscaler = StandardScaler()\rX_train = scaler.fit_transform(X_train)\rX_test = scaler.transform(X_test)\r3. Model   3.1 Predict the values after building a Machine learning model   Logistic regression is another technique borrowed by machine learning from the field of statistics.It is the go-to method for binary classification problems (problems with two class values). In this post we will discover the logistic regression algorithm for machine learning.This is a classification problem to predict whether somebody will face financial distress in the next two years. So, here we will train our data on a Logistic regression algorithm and try to correctly predict the class.\nlog_reg = LogisticRegression()\rlog_reg.fit(X_train,y_train)\ry_pred = log_reg.predict(X_test)\raccuracy = accuracy_score(y_test,y_pred)\r  3.2 Is our prediction right?   Simply, building a predictive model is not your motive. But, creating and selecting a model which gives high accuracy. Hence, it is crucial to check accuracy of the model and we consider different kinds of metrics to evaluate our models. The choice of metric completely depends on the type of model and the implementation plan of the model. After you are finished building your model, these metrics will help you in evaluating your model accuracy.We will print the classification report of our model. This can be done by the following code\nscore = roc_auc_score(y_pred , y_test)\ry_pred_proba = log_reg.predict_proba(X_test)[:,1]\rfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\rauc = metrics.roc_auc_score(y_test, y_pred_proba)\rplt.plot(fpr,tpr,label=\u0026quot;Logistic model, auc=\u0026quot;+str(auc))\rplt.xlabel('False Positive Rate')\rplt.ylabel('True Positive Rate')\rplt.legend(loc=4)\rplt.show()\rf1 = f1_score(y_test, log_reg.predict(X_test))\rprecision = precision_score(y_test, log_reg.predict(X_test))\rrecall = recall_score(y_test, log_reg.predict(X_test))\rroc_auc = roc_auc_score(y_test, log_reg.predict(X_test))\rprint ('Confusion_matrix' + '\\n', confusion_matrix(y_test, log_reg.predict(X_test)))\rprint ('Classification_report' + '\\n' + classification_report(y_test,y_pred))\rObservation:\n As we can see our model is not working well in calss 1 category because this is an imbalanced data we will now try to fix this. Auc metric helps us to determine if our model is working well in imabalnced class.\n 4. SMOTE   4.1 Balancing the dataset using SMOTE   As we can see that the dataset is not balanced and it shows that 93% of customers will not face financial distress.In this situation, the predictive model developed using conventional machine learning algorithms could be biased and inaccurate.This happens because Machine Learning Algorithms are usually designed to improve accuracy by reducing the error. Thus, they do not take into account the class distribution / proportion or balance of classes. If we train our model on such an imbalanced data we will get incorrect predictions. To overcome this there are different methods such undersampling, oversampling and SMOTE. We will be using the SMOTE technique.Check for different evaluation metrics.\ncount = y.value_counts()\rsmote = SMOTE(random_state=9)\rX_sample, y_sample = smote.fit_sample(X_train, y_train)\rsns.countplot(y_sample)\rObservation:\n As we can see now our dataset is balanced now. Lets try applying new model on this dataset.\n   4.2 Effect of applying SMOTE?   SMOTE is an over-sampling method. What it does is, it creates synthetic (not duplicate) samples of the minority class. Hence making the minority class equal to the majority class. After applying \u0026lsquo;SMOTE\u0026rsquo; we have balanced the data. We will use this balanced data for training our model and check whether the performance of our model has improved or not by comparing different evaluation parameters.\nlog_reg.fit(X_sample, y_sample)\ry_pred = log_reg.predict(X_test)\rscore = roc_auc_score(y_pred , y_test)\ry_pred_proba = log_reg.predict_proba(X_test)[:,1]\rfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\rauc = metrics.roc_auc_score(y_test, y_pred_proba)\rplt.plot(fpr,tpr,label=\u0026quot;Logistic model, auc=\u0026quot;+str(auc))\rplt.xlabel('False Positive Rate')\rplt.ylabel('True Positive Rate')\rplt.legend(loc=4)\rplt.show()\rf1 = f1_score(y_test, log_reg.predict(X_test))\rprecision = precision_score(y_test, log_reg.predict(X_test))\rrecall = recall_score(y_test, log_reg.predict(X_test))\r# roc_auc = roc_auc_score(y_test, log_reg.predict(X_test))\rprint('Confusion matrix' + '\\n' ,confusion_matrix(y_test, log_reg.predict(X_test)))\rprint ('Classification_report' + '\\n' + classification_report(y_test,y_pred))\rObservation:\n If you compare previos classification report to this one you can observe that our model has imporved pretty much for our minority class. F1 score of class 1 has been improved from 3% to 21% , thats quite an improvemnt!! See the power of SMOTE.\n 5. Random Forest Algorithm Random Forrest is a bagging technique which uses Decision Tree as the base model. The performance of our Logistic regression model has signifiacntly improved after balancing the data, lets check can we furthur improve it by using a Random Forrest model.\nrf = RandomForestClassifier(random_state=9)\rrf.fit(X_sample, y_sample)\ry_pred = rf.predict(X_test)\rf1 = f1_score(y_test, rf.predict(X_test))\rprecison = precision_score(y_test, rf.predict(X_test))\rrecall = recall_score(y_test, rf.predict(X_test))\rscore = roc_auc_score(y_test, rf.predict(X_test))\rprint ('Confusion_matrix' + '\\n',confusion_matrix(y_test, rf.predict(X_test)))\rprint ('Classification_report' + '\\n' + classification_report(y_test,y_pred))\rscore = roc_auc_score(y_pred , y_test)\ry_pred_proba = rf.predict_proba(X_test)[:,1]\rfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\rauc = metrics.roc_auc_score(y_test, y_pred_proba)\rplt.plot(fpr,tpr,label=\u0026quot;Random_forest model, auc=\u0026quot;+str(auc))\rplt.xlabel('False Positive Rate')\rplt.ylabel('True Positive Rate')\rplt.legend(loc=4)\rplt.show()\rObservation:\n As you can see our model has been improved.F1 score of class 1 increased from 21% to 33%.\n  Code and Data Full Code and Dataset can be found here\nConclusion and Future Work Congratulations if you have reached her :) You can try to increase the f1 score of minority class by hyperparameter tuning of random forest. Also you can try using xgboost. If You have any question regarding this blog feel free to contact me on my website\nReference  https://greyatom.com/ https://www.appliedaicourse.com/  ","date":"2020-01-11T00:00:00Z","permalink":"https://shubendu.github.io/p/financial-distress-prediction-using-smote/","title":"Financial Distress Prediction using SMOTE"}]